{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "857784b3",
   "metadata": {},
   "source": [
    "Authors: Vo, Huynh Quang Nguyen; Nguyen, Duc Huy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef53d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from utils import autograder_multivariate_calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1b826",
   "metadata": {},
   "source": [
    "# Note: This quiz use autograder\n",
    "1. To finish a coding task, you first complete the coding cell denoted as the `CODING SESSION`. Next, you run the coding cell and subsequently the grading cell denoted as `GRADING SESSION`. \n",
    "\n",
    "\n",
    "2. If your code does not have any errors, no notification will appear. Otherwise, an `assertError` appears and you are prompted to fix the code before continuing.\n",
    "\n",
    "***\n",
    "\n",
    "As an example, our task is to create the following matrix:\n",
    "$$\n",
    "A = \\begin{bmatrix} 3 & 4 & 5 \\\\ 3 & 4 & 5\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We enter the following code, then run the cell containing the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8199e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# CODING SESSION\n",
    "#\n",
    "A = np.array([[3,4,5],[3,4,5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c177da",
   "metadata": {},
   "source": [
    "We finally run the cell containing the autograder to finish the task. Again, if there is no error in our code, no notification will appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d258f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# GRADING SESSION\n",
    "#\n",
    "autograder_multivariate_calculus.example(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2187009c",
   "metadata": {},
   "source": [
    "However, if we change our code to a 'wrong one' and run the autograder, an AssertionError will appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77574afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Uncomment the following codes to see how the notification appears when our code is wrong.\n",
    "#\n",
    "\n",
    "#A = np.array([[1,2,3],[4,5,6]])\n",
    "#autograder_multivariate_calculus.example(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d2ca2f",
   "metadata": {},
   "source": [
    "# I. Derivatives of a function\n",
    "In this task, you will use the Sympy library to find the derivative of a function-under-interest, then evaluate this derivative at a point:\n",
    "\n",
    "1. $f(x) = x^{3/2} + \\pi x^2 + \\sqrt{7}$, $f'(x)$ evaluated at $x = 2$\n",
    "\n",
    "\n",
    "2. $f(x) = sin(x)e^{cos(x)}$, $f'(x)$ evaluated at $x = \\pi$\n",
    "\n",
    "\n",
    "3. $f(x) = e^{(x+1)^2}$, $f'(x)$ evaluated at $x = 1$\n",
    "\n",
    "\n",
    "4. $f(x) = x^2cos^3(x)$, $f'(x)$ evaluated at $x = \\pi$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ab48ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# CODING SESSION\n",
    "#\n",
    "x = symbols('x')\n",
    "\n",
    "f_1 = ...\n",
    "eval_1 = ...\n",
    "\n",
    "f_2 = ...\n",
    "eval_2 = ...\n",
    "\n",
    "f_3 = ...\n",
    "eval_3 = ...\n",
    "\n",
    "f_4 = ...\n",
    "eval_4 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff6c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# GRADING SESSION\n",
    "#\n",
    "autograder_multivariate_calculus.derivative(eval_1,eval_2,eval_3,eval_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6805d2c1",
   "metadata": {},
   "source": [
    "# II. Jacobians and Hessians\n",
    "In this practice, you will use the Sympy library to find the Jacobian/Hessian of a function-under-interest, then evaluate its Jacobian/Hessian at a point:\n",
    "\n",
    "1. $f(x,y,z) = x^2cos(y) + e^zsin(y)$, $J[f(x)]$ evaluated at $x = (\\pi,\\pi,1)$.\n",
    "\n",
    "\n",
    "2. $u(x,y) = x^2y - cos(x)sin(y)$ and $v(x,y) = e^{x +y}$. The Jacobian of vector-value functions $\\begin{bmatrix} J[u(x,y)] \\\\ J[v(x,y)] \\end{bmatrix}$ evaluated at $x = (0,\\pi)$.\n",
    "\n",
    "\n",
    "3. $f(x,y) = x^3cos(y) - xsin(y)$, $H[f(x,y)]$ evaluated at $x = (0,0)$.\n",
    "\n",
    "\n",
    "4. $f(x,y,z) = xycos(z)âˆ’sin(x)e^yz^3$, $H[f(x,y,z)]$ evaluated at $(x,y,z) = (0,0,0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# CODING SESSION\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# GRADING SESSION\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83431770",
   "metadata": {},
   "source": [
    "# III. Linear Regression with Gradient Descent\n",
    "1. In statistics, linear regression is a linear approach to model the relationship between a response and one or more explanatory variables\". Mathematically speaking, given a response variable $y$, an explanatory variable $x$, and an assummed linear relationship between $y$ and $x$: $y=mx+b$, our aim is to find $m$ and $b$ that best fits $x$ to $y$. \n",
    "\n",
    "\n",
    "2. Sometimes, we will find the equation for linear regression expressed as $y = mx_1 + x_0$ for the sake of programming. If $\\mathbf{X}$ is a multidimensional vector, this expression can be expressed as:\n",
    "$$\n",
    "y = x_0 + \\mathbf{M}\\mathbf{X}= x_0 + m_{1}x_{1} + m_{2}x_{2} + m_{3}x_{3} + ... + m_{n}x_{n}$.\n",
    "$$\n",
    "\n",
    "\n",
    "3. In Machine Learning, we often call: \n",
    "    * x: input data. \n",
    "    * y: output data.\n",
    "    * m: weight\n",
    "\n",
    "***\n",
    "\n",
    "In this exercise, you will see on how to solve a simple linear regression problem by using a very famous technique in Machine Learning called Gradient Descent. The steps to implement Gradient Descent are as follow: \n",
    "\n",
    "1. Initialize $m$ and $b$ by random numbers, or just $m=0$, $b=0$.\n",
    "\n",
    "\n",
    "2. Find $y_{pred} = mx + b$.\n",
    "\n",
    "\n",
    "3. Update $m$ and $b$ by using $m = m - \\alpha\\frac{\\partial f(m,b)}{\\partial m}$ and $b = b - \\alpha\\frac{\\partial f(m,b)}{\\partial b}$. \n",
    "\n",
    "In this step, you will need to find the partial derivatives $\\frac{\\partial f(m,b)}{\\partial m}$ and $\\frac{\\partial f(m,b)}{\\partial b}$ first. $f(m,b)$ is the cost function representing the difference between the predicted values and the actual (also known as the ground truth) values. Here, our cost function is 'One Half Mean Square Error':\n",
    "\n",
    "$$\n",
    "J(m,b) = \\frac{1}{2} \\times \\frac{1}{n}\\sum_{i=1}^n (y_{ipred}-y_i)^2 = \\frac{1}{2} \\times \\frac{1}{n}\\sum_{i=1}^n ((mx_i+b)-y_i)^2\n",
    "$$.\n",
    "\n",
    "The partial derivatives of our cost function are as follows:\n",
    "> * $\\frac{\\partial J(m,b)}{\\partial m} =  \\frac{1}{2} \\times \\frac{2}{n}\\sum_{i=1}^n x_i((mx_i+b)-y_i)=\\frac{1}{n}\\sum_{i=1}^n x_i((mx_i+b)-y_i)=\\frac{1}{n}\\sum_{i=1}^n x_i(y_{ipred}-y_i)$.\n",
    "\n",
    "> * $\\frac{\\partial J(m,b)}{\\partial b} =  \\frac{1}{2} \\times \\frac{2}{n}\\sum_{i=1}^n 1((mx_i+b)-y_i)=\\frac{1}{n}\\sum_{i=1}^n ((mx_i+b)-y_i)=\\frac{1}{n}\\sum_{i=1}^n (y_{ipred}-y_i)$.\n",
    "\n",
    "\n",
    "4. Repeat from step 2.\n",
    "\n",
    "<img src=\"images/gradient_descent.gif\" width=\"80%\">\n",
    "\n",
    "Figure 1: Visualization of the gradient descent algorithm for linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92531ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Read the .csv file and show the data:\n",
    "#\n",
    "data = pd.read_csv(\"./dataset/HourStudied_vs_ExamScore.csv\") \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a629233",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Assign the .csv data to X and y:\n",
    "# \n",
    "X = np.array(data['Hours Studied'])\n",
    "y = np.array(data['Percentage Score'])\n",
    "plt.scatter(X, y)\n",
    "plt.title('Time spent on studying vs. Exam Score')\n",
    "plt.xlabel('Hours Studied')\n",
    "plt.ylabel('Exam Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc82023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## \n",
    "# Check the size of dataset:\n",
    "#\n",
    "print('Shape of vector X:', X.shape)\n",
    "print('Shape of vector y:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d993d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Reshape the shape of vector for the purpose of mathematical operation:\n",
    "#\n",
    "X = X.reshape(X.shape[0],1)\n",
    "y = y.reshape(y.shape[0],1)\n",
    "print('Shape of vector X:', X.shape)\n",
    "print('Shape of vector y:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7c26fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Step 1: Initialize m and b:\n",
    "#\n",
    "m = 0\n",
    "b = 0\n",
    "\n",
    "for i in range(50):   \n",
    "    display.clear_output(wait= True)\n",
    "\n",
    "    ##\n",
    "    # Step 2: Find y_pred = mx + b:\n",
    "    #\n",
    "    y_pred = m*X + b\n",
    "        \n",
    "    ##\n",
    "    # Step 3: Update m and b using the Gradient Descent algorithm:\n",
    "    #\n",
    "    dm = np.mean((y_pred - y) * X)\n",
    "    db = np.mean(y_pred - y)\n",
    "    m = m - 0.005*dm\n",
    "    b = b - 0.005*db\n",
    "    \n",
    "    ##\n",
    "    # Step 4: Plot and repeat:\n",
    "    #\n",
    "    plt.scatter(X, y)\n",
    "    plt.plot(X, y_pred)\n",
    "    plt.title('Time spent on studying vs. Exam Score')\n",
    "    plt.xlabel('Hours Studied')\n",
    "    plt.ylabel('Exam Score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b51941",
   "metadata": {},
   "source": [
    "Using the example given above, solve the linear regression problem for the ``Xy_dataset.csv` dataset. Then, answer the following questions:\n",
    "\n",
    "1. Using the exact parameters above ($m = 0$, $b = 0$, `for i in range(50):`, `m = m - 0.005*dm`, `b = b - 0.005*db`), does your code work?\n",
    "\n",
    "\n",
    "2. For the part `m = m - 0.005*dm` and `b = b - 0.005*db`, try to change $0.005$ to some larger number (like $0.1$). What do you observe? Is it better now?\n",
    "\n",
    "\n",
    "3. Again, for the part `m = m - 0.005*dm` and `b = b - 0.005*db`, try to change $0.005$ to $0.2$. What do you see? (In fact, this effect is called overshoot. And the `0.005` is called 'learning rate'. Generally, we often start with a small learning rate (but not too small!). Technically speaking, the learning rate depends on the dataset. A more advanced technique is called 'learning rate scheduler' which will modify the learning rate on the fly instead of using the fixed one, but you don't need to worry about this for now.\n",
    "\n",
    "\n",
    "4. For the part `for i in range(50):`, change it to `for i in range(100):`. Also change the 'learning rate' to $0.1$. Does it work better now? (The `50` in `for i in range(50):` is actually called epochs, or a number of iteration. Next time, instead of writing `for i in range(50):`, we will write `for i in range(epochs):`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b61928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Read the .csv file:\n",
    "#\n",
    "data = pd.read_csv(\"./dataset/Xy_dataset.csv\") \n",
    "\n",
    "##\n",
    "# Assign CSV data to X and y. Noted that we only use 'Biking' and 'Heart Disease':\n",
    "#\n",
    "X = np.array(data['X'])\n",
    "y = np.array(data['y'])\n",
    "plt.scatter(X, y)\n",
    "plt.title('X vs. y')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75216590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Check the size of dataset:\n",
    "#\n",
    "print('Shape of vector X:', X.shape)\n",
    "print('Shape of vector y:', y.shape)\n",
    "\n",
    "##\n",
    "# Reshape to appropriate size:\n",
    "#\n",
    "X = X.reshape(X.shape[0],1)\n",
    "y = y.reshape(y.shape[0],1)\n",
    "print('Shape of vector X:', X.shape)\n",
    "print('Shape of vector y:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349bd1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "# CODING SESSION\n",
    "# 1.\n",
    "\n",
    "##\n",
    "# Step 1: Initialize m and b:\n",
    "#\n",
    "m = ...\n",
    "b = ...\n",
    "\n",
    "for i in range(...):\n",
    "    display.clear_output(wait = True)\n",
    "    ##\n",
    "    # Step 2: Find y_pred = mx + b:\n",
    "    #\n",
    "    ...\n",
    "    \n",
    "    ##\n",
    "    # Step 3: Update m and b using the Gradient Descent algorithm:\n",
    "    #\n",
    "    ...\n",
    "    \n",
    "    ##\n",
    "    # Step 4: Plot and repeat\n",
    "    #\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# GRADING SESSION\n",
    "#\n",
    "autograder_multivariate_calculus.question1(m,b,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a6a90c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## \n",
    "# CODING SESSION\n",
    "# 2.\n",
    "\n",
    "##\n",
    "# Step 1: Initialize m and b:\n",
    "#\n",
    "m = ...\n",
    "b = ...\n",
    "\n",
    "for i in range(...):\n",
    "    display.clear_output(wait = True)\n",
    "    ##\n",
    "    # Step 2: Find y_pred = mx + b:\n",
    "    #\n",
    "    ...\n",
    "    \n",
    "    ##\n",
    "    # Step 3: Update m and b using the Gradient Descent algorithm:\n",
    "    #\n",
    "    ...\n",
    "    \n",
    "    ##\n",
    "    # Step 4: Plot and repeat\n",
    "    #\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# GRADING SESSION\n",
    "#\n",
    "autograder_multivariate_calculus.question2(m,b,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf2fc8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## \n",
    "# CODING SESSION\n",
    "# 3.\n",
    "\n",
    "##\n",
    "# Step 1: Initialize m and b:\n",
    "#\n",
    "m = ...\n",
    "b = ...\n",
    "\n",
    "for i in range(...):\n",
    "    display.clear_output(wait = True)\n",
    "    ##\n",
    "    # Step 2: Find y_pred = mx + b:\n",
    "    #\n",
    "    ...\n",
    "    \n",
    "    ##\n",
    "    # Step 3: Update m and b using the Gradient Descent algorithm:\n",
    "    #\n",
    "    ...\n",
    "    \n",
    "    ##\n",
    "    # Step 4: Plot and repeat\n",
    "    #\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b4e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# GRADING SESSION\n",
    "#\n",
    "autograder_multivariate_calculus.question3(m,b,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f705d979",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## \n",
    "# CODING SESSION\n",
    "# 4.\n",
    "\n",
    "##\n",
    "# Step 1: Initialize m and b:\n",
    "#\n",
    "m = ...\n",
    "b = ...\n",
    "\n",
    "for i in range(...):\n",
    "    display.clear_ouput(wait = True)\n",
    "    ##\n",
    "    # Step 2: Find y_pred = mx + b:\n",
    "    #\n",
    "    ...\n",
    "    \n",
    "    ##\n",
    "    # Step 3: Update m and b using the Gradient Descent algorithm:\n",
    "    #\n",
    "    ...\n",
    "    \n",
    "    ##\n",
    "    # Step 4: Plot and repeat\n",
    "    #\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# GRADING SESSION\n",
    "#\n",
    "autograder_multivariate_calculus.question4(m,b,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6a4cfc",
   "metadata": {},
   "source": [
    "# Demo: Application of L1 Normalization in Data Preprocessing"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
