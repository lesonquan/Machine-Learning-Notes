{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c28307bc",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Linear Algebra\n",
    "Author: Vo, Huynh Quang Nguyen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044a239e",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ffb1b3",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Acknowledgements:\n",
    "The contents of this note are based on the lecture notes and the materials from the following source:\n",
    "1. _Mathematics for Machine Learning_ course given by Prof. David Dye, Dr Samuel J. Cooper, and Dr A. Freddie Page from Imperial College London, United Kingdom. \n",
    "Available in Coursera: [Mathematics for Machine Learning specialization](https://www.coursera.org/specializations/mathematics-machine-learning) \n",
    "\n",
    "\n",
    "2. _Mathematics for Machine Learning_ textbook by Prof. Marc Peter Deisenroth, Prof. A. Aldo Faisal, and Prof. Cheng Soon Ong. \n",
    "\n",
    "The book is freely available at [Mathematics for Machine Learning textbook](https://mml-book.github.io/) \n",
    "\n",
    "3. _Essential Math for Data Science in 6 Weeks_ webinar given by Dr Thomas Nield. \n",
    "\n",
    "Available in O'Reilly Learning: [Essential Math for Data Science in 6 Weeks webinar](https://learning.oreilly.com/attend/essential-math-for-data-science-in-6-weeks/0636920055929/0636920055928/)\n",
    "\n",
    "4. _Deep Learning_ textbook by Dr Ian Goodfellow, Prof. Yoshua Bengio, and Prof. Aaron Courville. The book is available for public access via a designated website: [Deep Learning textbook](https://www.deeplearningbook.org/)\n",
    "\n",
    "\n",
    "5. _ELEC-E3240 Photonics_ course given by Prof. Zhipei Sun and Huynh Quang Nguyen Vo (Teaching Assistant) from Aalto University, Finland.\n",
    "\n",
    "Additionally, the author would like to express his sincerest gratitude to:\n",
    "1. Prof. Alexander Jung (from Aalto University, Finland) for his easy-to-understand explanations on how to represent neural networks using matrices.\n",
    "\n",
    "\n",
    "2. Prof. Jeff Heaton (from Washington University of St. Louis, USA) for his explanation on the $L^{p}$ normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6eecb4",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Table of Contents\n",
    "1. [Introduction to Linear Algebra](#Section1) \n",
    "2. [Vector operations](#Section2) \n",
    "3. [Matrix operations](#Section3) \n",
    "4. [Solving systems of equations](#Section4)\n",
    "5. [Eigenvalues and Eigenvectors](#Section5)\n",
    "6. [Appendix](#Section6)\n",
    "7. [Suggestive Reading](#Section7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc07a10f",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## I. Introduction to Linear Algebra <a name = 'Section1'></a>\n",
    "\n",
    "### 1. Why we have to learn?\n",
    "\n",
    "1. Firstly, linear algebra can be found in many areas of science, technology, engineering, and data science. \n",
    "\n",
    "\n",
    "2. Secondly, linear algebra also serves as a crucial pillar in machine learning, data management, graphical modelling, and other computer science areas.\n",
    "\n",
    "\n",
    "3. Thirdly, modern computers model data as vectors and matrices to perform operations more effectively.\n",
    "\n",
    "\n",
    "4. Therefore, linear algebra is a **MUST** to advance your knowledge in machine learning, statistical modelling, or other areas in computer science.\n",
    "\n",
    "\n",
    "### 2. Applications of Linear Algebra\n",
    "\n",
    "As mentioned earlier, linear algebra serves as a backbone in many scientific fields, especially as one of the three main pillars in machine learning. Below are selected examples of how it is applied across various scientific fields.\n",
    "\n",
    "#### a) Optics\n",
    "\n",
    "<div>\n",
    "<img src=\"images/ray-transfer-matrix.png\" width=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 1: Ray transfer matrix analysis for imaging inside linear media. This method of image computation is widely employed when designing an optical system such as a microscope, a telescope, or a laser guidance system.\n",
    "\n",
    "#### b) Computer vision\n",
    "\n",
    "<div>\n",
    "<img src=\"images/perspective.png\" width=\"70%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 2: Perspective transformation from 3D into 2D (adapted from **Computer Vision: Algorithms and Applications** by Szeliski). This method of projection is popular in computer graphics when recreating 3D models in 2D environments.\n",
    "\n",
    "\n",
    "![SegmentLocal](images/2D_Convolution_Animation.gif)\n",
    "\n",
    "Figure 3: Operating principles of a 2-D convolution. The convolution operation serves as a backbone in mask-based image filtering, and convolutional neural network. \n",
    "\n",
    "#### c) Geography\n",
    "<div>\n",
    "<img src=\"images/geography.png\" width=\"50%\"/>    \n",
    "</div>\n",
    "\n",
    "Figure 4: Computation of an area of a polygon using coordinates. This method, which is often referred to as the shoelace algorithm or Gaussian area algorithm, is widely employed in computational geography for computing the area of a region.\n",
    "\n",
    "#### d) Signal Processing\n",
    "<div>\n",
    "    <img src=\"images/signal.png\" width=\"35%\" />\n",
    "    </div>\n",
    "\n",
    "Figure 5: Operating principles of a digital filter. A signal in the time-domain is transformed to the frequency domain using Fourier transform, then subjected to a digital filter to smoothen out, and finally converted back to the time domain. This concept of filtering in frequency-domain can also be applicable for image processing.\n",
    "\n",
    "![SegmentLocal](images/1D_convolution.gif)\n",
    "Figure 6: Operating principles of a 1-D convolution. To filter a signal in its time domain, we perform the convolution between the signal and a filter (also called a kernel). \n",
    "\n",
    "![SegmentLocal](images/Fourier_transform_time_and_frequency_domains.gif)\n",
    "Figure 7: Operating principles of a Fourier Transform. A signal in the time domain can be transformed to the frequency domain using Fourier transform. To filter a signal in its time domain, we perform the matrix multiplication between the signal and a digital filter.\n",
    "$$\n",
    "O(\\omega) = H(\\omega)\\bullet I(\\omega)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b5a4b",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## II. Vector operations <a name = \"Section2\"></a>\n",
    "\n",
    "### 1. What is a vector:\n",
    "1. Vectors are an arrow in space, with a specific direction and length. This can mean different things to different disciplines:\n",
    "   * Physics - direction and magnitude (e.g velocity)\n",
    "   * Computer Science - an array of data.\n",
    "   * Math - A direction and scale on a designated coordinate system.\n",
    "\n",
    "\n",
    "2. In the context of machine learning, we can formally define a vector as follows: a vector is a 1-D array of numbers, where each element is identified by an index.\n",
    "\n",
    "\n",
    "3. For a two-dimensional vector, we can think of it as a pair of numbers. For example, we can interpret the following vector $\\vec{v} = [3,4] $ as follows:\n",
    "    * This vector has the name of $\\vec{v}$.\n",
    "    * From the origin (0,0) we make 3 stops along the x-axis, and 4 steps up the y-axis. \n",
    "\n",
    "\n",
    "4. When defining vectors, they must start at the origin (for example $(x_0,y_0) = (0,0)$ for a 2-D space) and cannot arbitrarily start at any point in space. We also need to say what kind of numbers are stored in the vector. If each element is in $\\mathbb{R}$, and the vector has $n$ elements, then the\n",
    "vector lies in the set formed by taking the Cartesian product of $\\mathbb{R}$ n  times, denoted as $\\mathbb{R}^n$.\n",
    "$$\n",
    "$$\n",
    "<div>\n",
    "<img src=\"images/vector-representation.png\" width=\"100%\"/>\n",
    "</div>\n",
    "Figure 1: Examples of 2D vectors.\n",
    "\n",
    "\n",
    "5. Vectors can apply beyond just a 2-dimensional space. In practice, vectors can exist in any number of dimensions, although it gets hard to visualize outside of a 3-dimension space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a818b4",
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 5]\n",
      "(3,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "# DEMO: DECLARING A VECTOR\n",
    "#\n",
    "\n",
    "v = [3, 4, 5]\n",
    "v = np.array(v)     # We use this function to convert a Python list into a vector\n",
    "print(v)\n",
    "print(v.shape)      #We use this function to determine the dimension of our vector\n",
    "\n",
    "##\n",
    "# DEMO: VECTOR VISUALIZATION\n",
    "#\n",
    "origin = np.array([0,0])  \n",
    "v = np.array([3,2])\n",
    "\n",
    "# Creating plot\n",
    "plt.quiver(origin[0], origin[1], v[0], v[1], color='b', units='xy', scale=1)  \n",
    "plt.xlim(-2, 5)\n",
    "plt.ylim(-2, 2.5)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d78b7",
   "metadata": {
    "deletable": false
   },
   "source": [
    "### 2. Basic operations with vectors\n",
    "\n",
    "#### a) Vector-on-vector addition and subtraction\n",
    "1. To visually add these two vectors together, we connect one vector after the other and walk to the tip of the last vector. The point we end at is a new vector, the result of summing the two vectors. \n",
    "\n",
    "\n",
    "2. Because of the commutative nature of adding vectors, as it does not matter which order we add them.\n",
    "\n",
    "$$\n",
    "\\vec{v} + \\vec{w} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} + \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3. For subtraction, we can imagine it as a vector $\\vec{v}$ sums with a 'negative' vector $-\\vec{w}$ out of the original $\\vec{w}$. Noted that the 'negative' notion here means a vector having the same length but with an opposite direction.\n",
    "\n",
    "<div>\n",
    "<img src=\"images/vector-add.png\" width=\"100%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 2: An example of adding two vectors together by connecting one vector after the other and walking to the tip of the last vector.\n",
    "\n",
    "#### b) Scalar-on-vector Multiplication\n",
    "1. We can also grow/shrink a vector by multiplying (others referred to this as scaling, though it is a matter of word choice) with a scalar value.\n",
    "\n",
    "<div>\n",
    "<img src = \"images/vector-multiply.png\" width = \"80%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 3: Examples of scaling a vector by a scalar: positive scalars increase/decrease the length of a vector (a-b), while negative scalars not only increase/decrease but also reverse a direction of a vector (c). \n",
    "\n",
    "2. As an additional note, a scalar is just a single number, in contrast to most of the other objects studied in linear algebra, which are usually arrays of multiple numbers. When we introduce scalars, we need to specify what kind of number they are: be it $\\mathbb{N}$ (natural numbers), $\\mathbb{Z}$ (integers), $\\mathbb{R}$ (real numbers).\n",
    "\n",
    "#### c) Magnitude and Normalization\n",
    "1. A magnitude (length) of a vector can be computed as follows:\n",
    "$$\n",
    "||\\vec{v}|| = \\sqrt{v_1^2 + v_2^2 + ... + v_n^2}\n",
    "$$\n",
    "\n",
    "2. A vector can be normalized by dividing its elements by their magnitude:\n",
    "\n",
    "$$\n",
    "\\vec{v}_{norm} = \\frac{\\vec{v}}{||\\vec{v}||} = \\frac{1}{\\sqrt{v_1^2 + v_2^2 + ... + v_n^2}}\\begin{bmatrix} v_1\\\\ v_2 \\\\ ... \\\\v_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3. The above equation that computes the length of a vector is the so-called $L^2$ normalization also known as the **Euclidean** normalization. In practice, a vector can be subjected to $L^p$ normalization with $p\\ge1$:\n",
    "$$\n",
    "||\\vec{v}||_p = (\\sum_{i} |v_i|^p)^{1/p}\n",
    "$$\n",
    "\n",
    "4. In Machine Learning, we commonly use the $L^{1}$, $L^{2}$, and $L^{\\infty}$ normalizations. The reasons are as follows:\n",
    "\n",
    "***\n",
    "* Recall that the general normalization of a vector is:\n",
    "$$\n",
    "||\\vec{v}||_p = (\\sum_{i} |v_i|^p)^{1/p}\n",
    "$$\n",
    "\n",
    "* For $L^{1}$ normalization, we can rewrite the above expression as:\n",
    "\n",
    "$$\n",
    "||\\vec{v}||_1 = \\sum_{i} |v_i| = |v_1| + |v_2| + ... + |v_n|\n",
    "$$\n",
    "\n",
    "* We can see that $L^{1}$ normalization is the sum of all corresponding absolute values of every element in a vector. This is the most intuitive way to understand measure the distance between vectors and measure the magnitude of a vector. For example, consider we have a vector $\\vec{v} = [3, 4]$, the magnitude of this vector is computed as:\n",
    "\n",
    "$$\n",
    "||\\vec{v}||_1 = |3| + |4| = 7\n",
    "$$\n",
    "\n",
    "* This means, to measure the magnitude of this vector, we walk 3 steps along the x-axis, then 4 steps along the y-axis. Thus, the distance we have travelled is 7 (as seen in the below figure).\n",
    "\n",
    "<div>\n",
    "    <img src = \"images/L1_norm.jpg\" />\n",
    "    </div>\n",
    "\n",
    "* On the other hand, the most interesting property of this normalization is that: imagine we take all the elements of a vector divided by the norm, the sum of all the elements will be exactly one. Because of this interesting property, we usually use $L_1$ normalization to make our data less complex for a machine learning model to train.\n",
    "\n",
    "***\n",
    "* As mentioned above, the $L^{2}$ normalization also known as the Euclidean normalization is the most popular normalization we have and would ever see to compute the magnitude of a vector. Why? Because it is the shortest distance to go from one point to another.\n",
    "    \n",
    "    \n",
    "* Using the same example as in $L_1$ normalization, the $L_2$ normalization is computed as:\n",
    "\n",
    "$$\n",
    "||\\vec{v}||_2 = \\sqrt{|3|^2 + |4|^2} = 5\n",
    "$$\n",
    "\n",
    "* It is clearly that the normalization we just computed is the most direct route (as seen in the below figure).\n",
    "<div>\n",
    "    <img src = \"images/L2_norm.jpg\" />\n",
    "    </div>\n",
    "    \n",
    "* On the other hand, the most interesting property of this normalization is that: the outlier elements presented in the vector significantly affect the normalization because of the square operation. Thus, $L_2$ is one of the most effective ways to study the outliers in a vector.\n",
    "\n",
    "***\n",
    "* The $L^{\\infty}$ normalization gives us the largest magnitude (in absolute value) among each element of a vector:\n",
    "$$\n",
    "L^{\\infty} = max_{i}|x_i|\n",
    "$$\n",
    "\n",
    "\n",
    "* As an example, consider a vector $\\vec{v}= [-6, 4, 2]$, the $L^{\\infty}$ normalization of this vector is 6.\n",
    "\n",
    "\n",
    "* The most interesting property of this normalization is that only the largest element has any effect. So, let's say if our vector represents the cost of constructing a building, by minimizing $L^{\\infty}$ we are reducing the cost of the most expensive building.\n",
    "\n",
    "***\n",
    "\n",
    "#### d) Dot product and cross product\n",
    "1. Dot product, also known as scalar product or inner product, is a pair-wise operation between two vectors and returns a single number. Geometrically speaking in 2D and 3D, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them.\n",
    "\n",
    "$$\n",
    "\\vec{v} \\bullet \\vec{w} = ||\\vec{v}||_2||\\vec{w}||_2cos(\\vec{v},\\vec{w})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec{v} \\bullet \\vec{w} = v_1w_1 + v_2w_2 + ... + v_nw_n\n",
    "$$\n",
    "\n",
    "2. Cross product is the operation that takes two vectors and returns a vector that is perpendicular to the plane from the aforementioned vectors.\n",
    "$$\n",
    "\\vec{v} \\times \\vec{w} = ||\\vec{v}||_2||\\vec{w}||_2cos(\\vec{v},\\vec{w})\n",
    "$$\n",
    "\n",
    "<div>\n",
    "<img src =\"images/dot-cross-product.png\" width =\"80%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 4: Visualization of a dot product and a cross product between vector $\\vec{A}$ and $\\vec{B}$. The angle between $\\vec{A}$ and $\\vec{B}$ is $\\theta$. Notice the notion $\\vec{e}$ means the result of a cross product is a vector.\n",
    "\n",
    "3. Let's revisiting the dot product. We know that a dot product is an operation that does a multiply/add operation between elements in vectors and matrices. So exactly what does it try to accomplish?\n",
    "\n",
    "\n",
    "4. We can think of the dot product between two vectors as taking one vector’s length, and projecting another vector onto it and multiplying that resulting vector’s length.\n",
    "For example, we project $\\vec{w}$ directly onto $\\vec{v}$. The dot product between $\\vec{w}$ and $\\vec{v}$ is a scalar that is the product between the projection of $\\vec{w}$'s length and the $\\vec{v}$'s length.\n",
    "\n",
    "<div>\n",
    "<img src =\"images/dot-product.png\" width = 80%>\n",
    "</div>\n",
    "\n",
    "Figure 5: The operation of a dot product between two vectors.\n",
    " \n",
    "\n",
    "#### e) Scalar projection\n",
    "1. Scalar projects are the size of the “shadow” of a vector onto another vector, if we imagine the sun shining down perpendicular to the second vector. Vector projections are that shadow vector. As the names imply, scalar projects are scalars and vector projections are vectors.\n",
    "\n",
    "<div>\n",
    "<img src =\"images/scalar-projection.png\" width =\"30%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 6: An example of a vector $\\vec{s}$ projected onto another vector $\\vec{r}$ resulting a vector $\\vec{s_1}$.\n",
    "\n",
    "2. The scalar projection of $\\vec{s}$ onto $\\vec{r}$ is calculated as :\n",
    "\n",
    "$$\n",
    "\\frac{\\vec{r}\\bullet\\vec{s}}{||\\vec{r}||_2}\n",
    "$$ \n",
    "\n",
    "3. The vector projection is calculated as:\n",
    "\n",
    "$$\\vec{r}\\frac{\\vec{r}\\bullet\\vec{s}}{r^2}\n",
    "$$\n",
    "\n",
    "For example, we want to project on $\\vec{s} = \\begin{bmatrix} 10 \\\\ 5 \\\\ -6 \\end{bmatrix}$ onto $\\begin{bmatrix} 3 \\\\ -4 \\\\ 0 \\end{bmatrix}$.\n",
    "* The resulting scalar projection is $\\frac{\\vec{r}\\bullet\\vec{s}}{||\\vec{r}||_2} = \\frac{10}{5} = 2 $\n",
    "* The resulting vector projection is $\\vec{r}\\frac{\\vec{r}\\bullet\\vec{s}}{r^2} = \\begin{bmatrix} 3 \\\\ -4 \\\\ 0\\end{bmatrix} \\frac{10}{25} = \\begin{bmatrix} 6/5 \\\\ -8/5 \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "#### f) Basis vectors, basis decomposition and basis transformation\n",
    "1. A vector can be decomposed and represented by its basis vectors. For example, consider a vector $\\vec{v} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}$, it can be decompose into:\n",
    "$$\n",
    "\\vec{v} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} = 3\\vec{i} + 2\\vec{j} = 3\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + 2\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "2. Thus, we can understand that the basis vectors are the “reference points” we use to describe other vectors. A basis vector having a length of 1 and point in perpendicular positive directions is called the **standard basis vector**. In 2D, two **standard basis vectors** (one for the x-axis and the other for the y-axis) can be combined to form a so-called **identity matrix**.\n",
    "\n",
    "$$\n",
    "\\vec{i}\\vec{j} = \\begin{bmatrix} 1 & 0\\\\ 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The basis vector concept can be extended beyond 2D ($\\vec{i}$, $\\vec{j}$) to 3D ($\\vec{i}$, $\\vec{j}$, $\\vec{k}$), and to\n",
    "as many additional dimensions we need. As a result, we can expand the above **identity matrix** to a multi-dimensional **identity matrix**.\n",
    "\n",
    "$$\n",
    "\\mathbf{I} = \\begin{bmatrix} 1 & 0 & 0 & ... & 0 \\\\ 0 & 1 & 0 & ...  & 0 \\\\ 0 & 0 & 1 & ... & 0 \\\\ ... & ... & ... & ... & ... \\\\ 0 & 0 & 0 & ... & 1  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Overall, the basis vectors can be anything as long as they are **orthogonal** to each other, which means their dot product is zero.\n",
    "\n",
    "3. To conduct a basis decomposition of a vector $\\vec{v}$ onto a set of basis vectors $\\vec{b_1}$ and $\\vec{b_2}$, we can simply use:\n",
    "$$\n",
    "v'_{b_1} = \\frac{\\vec{b_1}\\bullet\\vec{v}}{b_1^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "v'_{b_2} = \\frac{\\vec{b_2}\\bullet\\vec{v}}{b_2^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec{v}'= \\begin{bmatrix} v'_{b_1} \\\\ v'_{b_2} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If the basis vectors are the standard basis vectors, then it is very straightforward. \n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"images/basis-vector.png\" width=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 7: Visualization of two standard basis vectors $\\vec{i}$ and $\\vec{j}$ (a), and how a vector is represented using two basis vectors (b).\n",
    "\n",
    "4. Basis transformation, to put it simply, is changing from one set of reference points to another set of reference points. This transformation usually involves a matrix.  \n",
    "\n",
    "<div>\n",
    "<img src=\"images/basis-transformation.png\" width=\"20%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 8: Visualization of two different sets of basis vectors. They can be transformed to one another by using basis transformation.\n",
    "\n",
    "#### d) Span and linearly independent\n",
    "1. Considering two vectors $\\vec{v}$ and $\\vec{w}$ fixed in two different directions, both vectors can be scaled and added to create **any** new vector $\\alpha\\vec{v} + \\beta\\vec{w}$. Thus, the entire space of vectors we can create from two vectors is known as **span**. Therefore, if the sum of these two vectors gives us access to all vectors in space, they are called **linearly independent**.\n",
    "\n",
    "![SegmentLocal](images/linear-combination.gif)\n",
    "\n",
    "Figure 9: Visualization of linear combination. Given a vector $\\vec{v}$ + $\\vec{w}$, a linear combination is defined when we create a new vector that is a sum of $\\vec{v}$ or $\\vec{w}$, or a sum of any arbitrary $\\alpha\\vec{v}$ and $\\beta\\vec{v}$ (adapted from **Essence of Algebra** video playlist from _3 Blue 1 Brown_ Youtube channel). \n",
    "\n",
    "\n",
    "![SegmentLocal](images/span.gif)\n",
    "\n",
    "Figure 10: Visualization of span. Given a vector $\\vec{v}$ + $\\vec{w}$, a span is defined as all the possible combination of $\\alpha\\vec{v}$ and $\\beta\\vec{v}$ (adapted from **Essence of Algebra** video playlist from _3 Blue 1 Brown_ Youtube channel). \n",
    "\n",
    "2. Characteristics of linear independency:\n",
    "    * Considering a 3-D vector space, if two vectors are linearly dependent (share a direction) but a third one is linearly independent of the other two, the span will be a flat plane in space. \n",
    "    * If all three vectors were linearly dependent, their span would only be a line in space. \n",
    "    * If all three vectors were linearly independent, their span would be a 3-D space.\n",
    "    * This concept applies to any number of dimensions, not just 2-D or 3-D.\n",
    "    \n",
    "<div>\n",
    "<img src=\"images/linear-dependent.png\" width = \"50%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 11: An examples of three linearly independent vectors forming a span that is a 3D space (a); if one or two of them are linear dependent, their span will be a flat plane in space (b-c).\n",
    "\n",
    "\n",
    "3. We can determine whether a set of vectors are linear independent using the following steps:\n",
    "    * For example, we have three vectors to consider $\\vec{v}_1 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 5 \\end{bmatrix} $, $\\vec{v}_2 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 8 \\end{bmatrix} $, $\\vec{v}_3 = \\begin{bmatrix} 4 \\\\ -1 \\\\ 0 \\end{bmatrix} $. To determine the linear independecy, we need to find a set of scalars that fulfills\n",
    "$$\n",
    "c_1 \\begin{bmatrix} 0 \\\\ 1 \\\\ 5 \\end{bmatrix} + c_2 \\begin{bmatrix} 1 \\\\ 2 \\\\ 8 \\end{bmatrix} + c_3 \\begin{bmatrix} 4 \\\\ -1 \\\\ 0 \\end{bmatrix} = 0 \n",
    "$$\n",
    "    * After solving this, the only set of scalars that fulfills this equation is $c_1 = c_2 = c_3 = 0$. Therefore, we can conclude that these vectors are linear indepedent.\n",
    "    * An easier method is forming a matrix containing the vectors-under-interest, then finding its matrix determinant. If the determinant is zero, then some or all vectors-under-interest are linear dependents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5799771a",
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 1]\n",
      "[-4.5 -3. ]\n",
      "32\n",
      "[-3  6 -3]\n",
      "[0.4, 2.2]\n",
      "These vectors are linear independent.\n"
     ]
    }
   ],
   "source": [
    "v = np.array([3, 2])\n",
    "w = np.array([2, -1])\n",
    "\n",
    "##\n",
    "# DEMO: VECTOR-ON-VECTOR ADDITION\n",
    "#\n",
    "add = v + w\n",
    "print(add)\n",
    "\n",
    "##\n",
    "# DEMO: VECTOR-ON-VECTOR MULTIPLICATION\n",
    "#\n",
    "mul = -1.5 * v\n",
    "print(mul)\n",
    "\n",
    "##\n",
    "# DEMO: DOT PRODUCT AND CROSS PRODUCT\n",
    "#\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([4, 5, 6])\n",
    "dot = np.dot(x,y)\n",
    "cross = np.cross(x,y)\n",
    "print(dot)\n",
    "print(cross)\n",
    "\n",
    "##\n",
    "# DEMO: BASIC DECOMPOSITION\n",
    "#\n",
    "i_hat = np.array([3, 4])\n",
    "j_hat = np.array([4, -3])\n",
    "v = np.array([10,-5])\n",
    "\n",
    "w = [np.dot(i_hat,v) / np.linalg.norm(i_hat) ** 2, \n",
    "     np.dot(j_hat,v) / np.linalg.norm(j_hat)** 2]\n",
    "print(w)\n",
    "\n",
    "##\n",
    "# DEMO: LINEAR INDEPENDENCE\n",
    "#\n",
    "v_1 = np.array([0,1,5])\n",
    "v_2 = np.array([1,2,8])\n",
    "v_3 = np.array([4,-1,0])\n",
    "\n",
    "v = np.array([v_1, v_2, v_3]).transpose()\n",
    "result = np.linalg.det(v)\n",
    "\n",
    "if not result == 0:\n",
    "    print('These vectors are linear independent.')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dad36c",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## II. Matrix operations <a name = \"Section3\" ></a>\n",
    "\n",
    "### 1. What is a matrix:\n",
    "1. A matrix is like a vector (or collection of vectors), and it can have multiple rows and columns. \n",
    "\n",
    "\n",
    "2. In the context of machine learning, we can formally define a matrix as follows: a matrix is a 2-D array of numbers, so each element is identiﬁed by two indices instead of just one.  If a real-valued matrix A has a height of m and a width of n, then we say that $A ∈ \\mathbb{R}^{m×n}$.\n",
    "\n",
    "\n",
    "3. In practice, matrix is a convenient way to package data. For example, it is more convenient to represent a system of equations as matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94478743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4 5]\n",
      " [6 7 8]]\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# DEMO: DECLARE A MATRIX\n",
    "#\n",
    "v = [[3,4,5], [6,7,8]]\n",
    "v = np.array(v)\n",
    "print(v)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37c6c3",
   "metadata": {
    "deletable": false
   },
   "source": [
    "### 2. Basic matrix operations:\n",
    "\n",
    "#### a. Matrix-on-vector multiplication\n",
    "1. Recall the basis transformation, the basis transformation is technically speaking a matrix-on-vector multiplication.\n",
    "$$\n",
    "\\begin{bmatrix} x_{new} \\\\ y_{new} \\end{bmatrix} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} x_{old} \\\\ y_{old} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "    * For example, finding an image (output) of an object formed by an optical system is just applying matrix-vector multiplication. Here, our vectors are an object and its corresponding image, and the matrix is the transformation matrix (**M**) of the system.\n",
    "    * Another example is vector rotation by an angle of $\\theta$ described as: $\\vec{v} = \\vec{u}\\begin{bmatrix} cos\\theta & -sin\\theta \\\\ sin\\theta  & cos\\theta \\end{bmatrix}$\n",
    "\n",
    "<div>\n",
    "<img src =\"images/matrix-vector.png\" width=\"30%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 1: An example of matrix-on-vector multiplication: finding the output image of an object created by an optical system (adapted from **Fundamentals of Photonics** by Saleh et al.)\n",
    "\n",
    "<div>\n",
    "<img src=\"images/basis.png\" width = \"30%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 2: Rotation of a vector $\\vec{u}$ into a vector $\\vec{v}$ by an angle $\\theta$.\n",
    "\n",
    "\n",
    "2. Matrix-vector multiplication is done by computing the dot product between a matrix and a vector:\n",
    "\n",
    "<div>\n",
    "<img src = \"images/dot-produc-matrix.png\" width= \"50%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 3: How the dot product between a matrix and a vector works.\n",
    "\n",
    "3. Using the concept of matrix-on-vector multiplication, we can describe a wide variety of transformations such as rotations, reflections, dilations, and much more.\n",
    "\n",
    "\n",
    "4. So why do we care much about matrix-on-vector multiplication? The reasons are as follows:\n",
    "\n",
    "***\n",
    "* Considering one of the pinnacles of machine learning models that is the neural networks, here, we consider the simplest one: a neural network with two neurons and one output (as seen in the below figure).\n",
    "<div>\n",
    "    <img src = \"images/neural_network_1.png\" width = 50%/>\n",
    "    </div>\n",
    "* We can express the output of this network as follows:\n",
    "$$\n",
    "O = 0.5\\times0.3 + 0.2\\times0.2 = 0.19\n",
    "$$\n",
    "\n",
    "* Now what if our neural network has two neurons and two output? Instead of going through each node, we can simply represent the outputs using the matrix-on-vector multiplication (as seen in the below figure).\n",
    "<div>\n",
    "    <img src = \"images/neural_network_2.png\" width = 50%/>\n",
    "    </div>\n",
    "***\n",
    "\n",
    "#### b. Linear Transformation\n",
    "\n",
    "1. To make things simple, let's consider the rotation of basis $(\\vec{u_1},\\vec{u_2})$ to the new basis $(\\vec{v_1},\\vec{v_2})$ by an angle $\\theta$. Here, the basis notation means 'a reference system' consisting of 'reference points' that are our basis vectors.\n",
    "\n",
    "<div>\n",
    "<img src = \"images/linear_transform.png\" width= \"25%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 4: An example of the linear transformation: the rotation of a vector $\\vec{x}$ by an angle of $\\theta$ to get a vector $\\vec{y}$.\n",
    "\n",
    "2. We know that this basis rotation is simply put a basis transformation:\n",
    "$$\n",
    "(\\vec{v_1},\\vec{v_2})  =  (\\vec{u_1},\\vec{u_2}) \\begin{bmatrix} cos\\theta & -sin\\theta \\\\ sin\\theta  & cos\\theta \\end{bmatrix}\n",
    "$$    \n",
    "\n",
    "3. Therefore, we can simply describe this using this formula:\n",
    "$$\n",
    "(\\vec{v_1},\\vec{v_2})  =  (\\vec{u_1},\\vec{u_2})(\\mathbf{u}→\\mathbf{v})\n",
    "$$\n",
    "\n",
    "4. Because of the basis transformation, we can describe a changes of a coordinates of a vector from an old basis $(\\vec{u_1},\\vec{u_2})$ to a new basis $(\\vec{v_1},\\vec{v_2})$ as: \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} x_{new} \\\\ y_{new} \\end{bmatrix} = \\begin{bmatrix} cos\\theta & -sin\\theta \\\\ sin\\theta  & cos\\theta \\end{bmatrix} \\begin{bmatrix} x_{old} \\\\ y_{old} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "or in this formula:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{\\mathbf{v}} = (\\mathbf{u}→\\mathbf{v})\\mathbf{x}^{\\mathbf{u}}\n",
    "$$\n",
    "\n",
    "\n",
    "5. Next, imagine that we fix our old basis $(\\vec{u_1},\\vec{u_2})$ and now represent all vectors with this basis. So, we have a vector $\\vec{x}$ then we rotate it by an angle $\\theta$ to transform into a vector $\\vec{y}$. Now, instead of describing the vector $\\vec{y}$ with a new basis $(\\vec{v_1},\\vec{v_2})$, we describe that vector with our own fixed basis. This description is called linear transformation, and can be simply described with the formula:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}^{\\mathbf{u}} = (\\mathbf{u}→\\mathbf{v})\\mathbf{x}^{\\mathbf{u}}\n",
    "$$\n",
    "\n",
    "6. According to what we have analyzed so far, we can think of linear transformation as tracking where the basis vectors land, and then we can use that to transform a vector also. This concept of seeing where the basis vectors land is important because it allows us not just to create vectors but also transform existing vectors.\n",
    "    * For example, we have this vector $\\vec{v} = \\begin{bmatrix} 4 \\\\ 2 \\end{bmatrix}$, and we transformed it somehow and it became $\\vec{w}$. The basis vectors $\\vec{i}$ and $\\vec{j}$ now land at $\\begin{bmatrix} -0.25 \\\\ 0 \\end{bmatrix}$ and $\\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}$, respectively. We can describe the new vector with our fixed basis ($\\vec{i}$, $\\vec{j}$) as:\n",
    "\n",
    "$$\n",
    "\\vec{w} = \\begin{bmatrix} -0.25 & 0 \\\\ 0 & -1 \\end{bmatrix} \\vec{v} = \\begin{bmatrix} -1 \\\\ -2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "$$\n",
    "\n",
    "<div>\n",
    "<img src=\"images/lineartransformation.png\" width = \"100%\" />\n",
    "</div>\n",
    "\n",
    "Figure 4: An example of the linear transformation: linear transformation is tracking where the basis vectors land and use them to describe a transformed vector (a-b); using this concept, we can describe nearly almost all typical linear transformations such as sheer, rotation, flip, scale, etc.\n",
    "\n",
    "![SegmentLocal](images/linear-transformation.gif)\n",
    "\n",
    "Figure 5: Visualization of linear transformation. Imagine we have space $\\mathbb{T}$ having a grid inside, and we apply a linear transformation to $\\mathbb{T}$. The result is a new space $\\mathbb{T}'$ where the lines of the inside grid still retain their straightness. \n",
    "\n",
    "#### c. Determinant\n",
    "1. When we perform linear transformations, we are expanding or reducing space, and the degree of this expansion or reduction is called **determinant**. In other words, **determinants** describe how much a sampled area in vector space changes in scale with linear transformations.\n",
    "\n",
    "![SegmentLocal](images/determinant-1.gif)\n",
    "\n",
    "Figure 6: Visualizaion of a matrix, says $\\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix}$. We can visualize this matrix as a linear transformation of a unit matrix $\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$ by first walking 3 steps on the x-axis then 2 steps on the y-axis (adapted from **Essence of Algebra** video playlist from _3 Blue 1 Brown_ Youtube channel).\n",
    "\n",
    "![SegmentLocal](images/determinant-2.gif)\n",
    "\n",
    "Figure 7: Visualization of a determinant: as mentioned above, we says the $\\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix}$ is a linear transformation of a unit matrix $\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$. The determinant is the magnitude of this transformation, which is illustrated here as the new area created by the given matrix (adapted from **Essence of Algebra** video playlist from _3 Blue 1 Brown_ Youtube channel).\n",
    "\n",
    "2. Characteristics of determinant:\n",
    "    * Scaling will increase or decrease the determinant, as that will increase/decrease the sampled area. Shear or rotation do not affect the magnitude of the determinant.\n",
    "    * When the orientation flips (for example, $\\vec{i}$ and $\\vec{j}$ swap clockwise positions) then the determinant will be negative.\n",
    "    * When a determinant is 0, that means the transformation is linearly dependent and has squished all of space into a line. Because at this point, there is no area! This is why testing whether a determinant is zero tells us about linear dependency.\n",
    "    * As usual, the determinant extends to 3 or more dimensions. At higher dimensions, it becomes a matter of visualizing a sampled volume scaling, rotating, sheering, and flipping.\n",
    "    \n",
    "<div>\n",
    "<img src=\"images/determinants.png\" width=\"100%\">\n",
    "</div>\n",
    "\n",
    "Figure 8: Determinants and their characteristics: a determinant is simply how much a sampled area in vector space changes in scale with linear transformations (a); shear or rotation do not affect the determinant (b-c); if a determinant is zero, then the sampled area formed by two vectors is zero meaning they are linearly dependent.\n",
    "\n",
    "#### d. Matrix-on-Matrix Multiplication\n",
    "1. We can think of matrix multiplication as applying multiple transformations to a vector space. In other words, we can think of each transformation matrix as a function, where we apply from the inner-most and then outwards. \n",
    "\n",
    "For example, we have a vector $\\vec{v}$, and we want to apply the rotation then a sheer transformations to this vector:\n",
    "* The rotation transformation is described as: $\\mathbf{r} = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$.\n",
    "* The shear transformation is described as: $\\mathbf{s} = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$.\n",
    "* The combined rotation and shear is: $\\mathbf{M} = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$. Notice the order of the matrices.\n",
    "* Therefore, the new vector $\\vec{w}$ is:\n",
    "$\\vec{w} = \\begin{bmatrix} x_{new} \\\\ y_{new} \\end{bmatrix} = M \\begin{bmatrix} x_{old} \\\\ y_{old} \\end{bmatrix} $\n",
    "\n",
    "<div>\n",
    "<img src=\"images/matrix-matrix-mul.png\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "Figure 9: An example of how rotation and shear transformations look like. A combined transformation of rotation and shear is simply matrix-matrix multiplication.\n",
    "\n",
    "#### e. Special types of matrices:\n",
    "1. A tensor is an array of numbers arranged on a regular grid with a variable number of axes is known as a tensor. We identify the element of a tensor $\\mathbf{A}$ at coordinates $(i,j,k)$ by writing $\\mathbf{A}_{i,j,k}$. \n",
    "\n",
    "<div>\n",
    "    <img src=\"images/tensor.png\" width = 30%/>\n",
    "    </div>\n",
    "\n",
    "Figure 10: An example of a tensor: an RBG image. An RBG image consists of three channels (Red, Green, and Blue), and the number of channels corresponds to the number of axes we have. Moreover, each channel is just an array with a height m and a width n, and we usually refer to the channel dimension as resolution. \n",
    "\n",
    "\n",
    "2. An identity matrix is formally defined as a matrix that does not change any vector when we multiply that vector by that matrix. As mentioned above, an identity matrix is simply all of the entries along the main diagonal are 1, while all of the other entries are zero. Noted that an identity matrix is a square matrix, meaning its height is equal to its width.\n",
    "\n",
    "$$\n",
    "∀\\vec{x} ∈ \\mathbb{R}^n, \\mathbf{I}\\vec{x} = \\vec{x}.\n",
    "$$\n",
    "\n",
    "3. Consider a matrix $\\mathbf{A}$, the matrix inverse of $\\mathbf{A}$ is deﬁned as the matrix such that:\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n",
    "$$\n",
    "\n",
    "\n",
    "4. A diagonal matrix is a matrix that consists mostly of zeros and has non-zero entries only along the main diagonal. One example of the diagonal matrix is the identity matrix.\n",
    "\n",
    "\n",
    "5. A symmetric matrix is any matrix that is equal to its transpose. Noted that transpose is an operation that flips the columns and the rows of a matrix along its diagonal line.\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{A}^T\n",
    "$$\n",
    "\n",
    "![SegmentLocal](images/transpose.gif)\n",
    "\n",
    "Figure 11: Visualization of the transpose operation for a matrix $\\mathbf{A} ∈ \\mathbb{R}^{m\\times n} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ff3d971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 -1]\n",
      " [ 1  0]]\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# DEMO: MATRIX-ON-MATRIX MULTIPLICATION\n",
    "#\n",
    "R = np.array([[0, -1],[1,0]])\n",
    "S = np.array([[1,1],[0,1]])\n",
    "\n",
    "M = np.dot(S,R)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ae2bbf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## IV. Solving a system of linear equations <a name = \"Section4\"></a>\n",
    "1. We can represent a system of linear equations as a set of matrices.\n",
    "\n",
    "For example, we have a system of equation like this:\n",
    "$$\n",
    "4x + 2y + 4z = 44\n",
    "$$\n",
    "$$\n",
    "5x + 3y + 7z = 56\n",
    "$$\n",
    "$$\n",
    "9x + 3y + 6z = 72\n",
    "$$\n",
    "* We need to solve for x, y and z so we can rewrite our system as: $ \\mathbf{A} = \\begin{bmatrix} 4 & 2 & 4 \\\\ 5 & 3 & 7 \\\\ 9 & 3 & 6 \\end{bmatrix}$, $\\mathbf{B} = \\begin{bmatrix} 44 \\\\ 56 \\\\ 72 \\end{bmatrix}$, $\\mathbf{C} = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}$\n",
    "* We know that: $\\mathbf{A}\\mathbf{X} = \\mathbf{B}$. Therefore, we can rewrite $\\mathbf{X}$ as function between $\\mathbf{A}$ and $\\mathbf{B}$ as: $\\mathbf{X} = \\mathbf{A}^{-1}\\mathbf{B}$, with $\\mathbf{A}^{-1}$ is the inverse matrix of $\\mathbf{A}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf54ab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2. 34. -8.]\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# DEMO: SOLVING A SYSTEM OF LINEAR EQUATIONS:\n",
    "#\n",
    "A = np.array([[4,2,4],[5,3,7],[9,3,6]])\n",
    "B = np.array([44,56,72]).transpose()\n",
    "X = np.dot(np.linalg.inv(A),B)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49333492",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## V. Eigen values and eigen vectors <a name = \"Section5\"></a>\n",
    "\n",
    "###  1. Matrix decomposition\n",
    "1. It is the method of breaking up a matrix into its basic components. Matrix decomposition is helpful for tasks like finding inverse matrices, calculating determinants, as well as linear regression.\n",
    "\n",
    "\n",
    "2. There are many ways to decompose a matrix, but the most common method is eigendecomposition, which is often used for machine learning and Principal Component Analysis (PCA). \n",
    "\n",
    "### 2. Eigen decomposition\n",
    "1. For a square matrix **A**, an eigenvector ($\\vec{v}$) and eigenvalue ($\\lambda$) make this equation true:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\vec{v} = \\lambda\\vec{v}\n",
    "$$\n",
    "\n",
    "For example, we have a square matrix **A** = $\\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix}$. Its eigenvector and eigenvalue is $\\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix}$ and 6, respectively.\n",
    "\n",
    "* Compute the left-hand size (LHS) of the equation: $\\mathbf{A}\\vec{v} = \\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24 \\end{bmatrix} $\n",
    "* Compute the right-hand size (RHS) of the equation: $\\lambda\\vec{v} = 6 \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24 \\end{bmatrix} $\n",
    "* Therefore, the equation is fulfilled.\n",
    "\n",
    "\n",
    "2. To put it simply, eigendecomposition is a type of transformation to find the eigenvalues and eigenvectors.\n",
    "\n",
    "\n",
    "3. We assume that the eigenvector is non-zero.\n",
    "\n",
    "<div>\n",
    "    <img src = \"images/eigenvalue_meme.jpg\" width = 50%>\n",
    "    </div>\n",
    "    \n",
    "Figure 1: A fun joke about the eigenvalue. In the movie _Avengers: Endgame_, our hero Tony Stark (Iron Man) computed the eigenvalue and the spectral transform (also known as the Fourier Transform) of a Mobius Strip and discovered time travel was indeed possible.\n",
    "\n",
    "### 3. Finding eigen parameters:\n",
    "1. First, we find the eigenvalue by manipulating our equation:\n",
    "$$\n",
    "\\mathbf{A}\\vec{v} = \\lambda\\vec{v}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\vec{v} - \\lambda I\\vec{v} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{det}(\\mathbf{A} - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "For example, we have a square matrix **A** = $\\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix}$, then the final equation becomes:\n",
    "$$\n",
    "\\mathbf{det}(\\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} - \\lambda\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{det}(\\begin{bmatrix} -6 - \\lambda & 3 \\\\ 4 & 5 - \\lambda \\end{bmatrix}) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "(-6 -\\lambda)(5-\\lambda) - 3\\times4 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda^2 + \\lambda - 42 = 0\n",
    "$$\n",
    "\n",
    "From here, we get two possible eigenvalues being -7 and 6.\n",
    "\n",
    "2. Using the eigenvalues, we can calculate our respective eigenvectors by substituting them with the original equation.\n",
    "\n",
    "Using the above example, for the eigenvalue of 6 we have:\n",
    "$$\n",
    "\\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = 6 \\begin{bmatrix} x \\\\ y \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} - 6\\begin{bmatrix} 1 & 0 \\\\ 0 & 1  \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} -12 & 3 \\\\ 4 & -1 \\end{bmatrix}\\begin{bmatrix} x \\\\ y \\end{bmatrix} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### 4. Why eigen parameters?\n",
    "1. Finding the eigen parameters means we are finding a vector that when our matrix-under-interest multiplies with the former, the matrix does not change its direction!\n",
    "\n",
    "\n",
    "2. The only thing that changes is the 'size' of the matrix, for example:\n",
    "    * 1 means no change,\n",
    "    * 2 means doubling in length,\n",
    "    * −1 means pointing backwards along the eigenvalue's direction.\n",
    "\n",
    "![SegmentLocal](images/Eigenvectors.gif)\n",
    "\n",
    "Figure 2: An example of how eigenvectors operate: when a matrix is transformed using an eigenvector (blue vectors), the only thing that changes is its 'size'. The same goes when this matrix is transformed by another eigenvector (purple ones).\n",
    "\n",
    "3. As an example, considering a vector $\\vec{v} = \\begin{bmatrix}  1 \\\\ 0 \\end{bmatrix} $ and a vector $\\vec{w} = \\begin{bmatrix}  0 \\\\ 1 \\end{bmatrix}$. We denote the sum of these vectors to be a vector $\\vec{m_1} = \\vec{v} + \\vec{w} = \\begin{bmatrix}  1 \\\\ 1 \\end{bmatrix}$, then define a span of all vectors $\\vec{m_1}$ to be $\\alpha \\vec{v} + \\beta \\vec{w}$ ($\\forall \\alpha,\\beta \\ge 1$). Then, imagine we have a linear transformation for our vector $\\vec{m_1}$ denoted as $\\mathbf{T} = \\begin{bmatrix} 3 & 1 \\\\ 0 & 2\\end{bmatrix}$. When we apply this transformation, we can see that $\\vec{m_1}$ is knocked out of its original span.\n",
    "\n",
    "![SegmentLocal](images/eigen_meaning_1.gif)\n",
    "\n",
    "Figure 3: Explanation on eigenvalues and eigenvectors: considering a vector $\\vec{m_1}$ (yellow) that is a linear combination of the unit vectors $\\vec{v}$ and $\\vec{w}$ (red-green), and has a span (pink). When a linear transformation $\\mathbf{T}$ is applied to $\\vec{m}$, we can see that the vector gets knocked off its span (adapted from **Essence of Algebra** video playlist from _3 Blue 1 Brown_ Youtube channel).\n",
    "\n",
    "4. Now, considering a different vector $\\vec{v}' = \\begin{bmatrix}  1 \\\\ 0 \\end{bmatrix} $ and a vector $\\vec{w}' = \\begin{bmatrix}  0 \\\\ -1 \\end{bmatrix}$. We denote the sum of these vectors to be a vector $\\vec{m_2} = \\vec{v} + \\vec{w} = \\begin{bmatrix}  1 \\\\ 1 \\end{bmatrix}$, then define a span of all vectors $\\vec{m_2}$ to be $\\alpha \\vec{v}' + \\beta \\vec{w}'$ ($\\forall \\alpha,\\beta \\ge 1$). We can see that when we apply the same transformation $\\mathbf{T}$ to $\\vec{m_2}$, it does not get knocked out of its span. The only thing changes is the size of $\\vec{m}$ as the former increases by a factor of 2. Thus, we call $\\vec{m_2}$ the eigenvector of $\\mathbf{T}$, and 2 is the eigenvalue of $\\mathbf{T}$.\n",
    "\n",
    "![SegmentLocal](images/eigen_meaning_2.gif)\n",
    "\n",
    "Figure 4: Explanation on eigenvalues and eigenvectors: considering a vector $\\vec{m_2}$ (yellow) that is a linear combination of the unit vectors $\\vec{v}'$ and $\\vec{w}'$ (red-green) -noted that the vector $\\vec{w}'$ is the negative of vector $\\vec{w}$, and has a span (pink). When a linear transformation $\\mathbf{T}$ is applied to $\\vec{m_2}$, we can see that the vector remains on its span, and $\\vec{m_2}$ is scaled by a factor of 2. Thus, we called $\\vec{m_2}$ to be the eigenvector and 2 to be the eigenvalue of $\\mathbf{T}$ (adapted from **Essence of Algebra** video playlist from _3 Blue 1 Brown_ Youtube channel).\n",
    "\n",
    "### 5. Reconstructing a matrix from eigen parameters.\n",
    "1. From the original equation, we can reconstruct a matrix from eigen parameters using:\n",
    "\n",
    "$$\n",
    "A = Q\\Lambda Q^{-1}\n",
    "$$\n",
    "in which Q is the matrix of eigenvectors, $\\Lambda$ is the eigenvalues in diagonal form, and $Q^{-1}$ is the inverse of the matrix.\n",
    "\n",
    "\n",
    "2. For example, we have a matrix **A** = $\\begin{bmatrix} 1 & 2 \\\\ 4 & 5 \\end{bmatrix}$, and we manage to find its eigenvalues to be -0.464 and 6.464, its eigenvectors to be $\\begin{bmatrix} 0.806 \\\\ 0.343\\end{bmatrix}$ and $\\begin{bmatrix} 0.59 \\\\ -0.939 \\end{bmatrix}$, respectively.\n",
    "\n",
    "* We rewrite our eigenvalues and eigenvectors. For eigenvalues: $\\begin{bmatrix} -0.464 \\\\ 6.464 \\end{bmatrix}$, while for eigenvectors: $\\begin{bmatrix} 0.806 & 0.343 \\\\ 0.59 & -0.939\\end{bmatrix}$ \n",
    "* We apply the equation to reconstruct **A**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57960fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [4 5]]\n",
      "[-0.46410162  6.46410162]\n",
      "[[-0.80689822 -0.34372377]\n",
      " [ 0.59069049 -0.9390708 ]]\n",
      "[[1. 2.]\n",
      " [4. 5.]]\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# DEMO: FINDING EIGEN PARAMETERS\n",
    "#\n",
    "A = [[1,2], [4,5]]\n",
    "A = np.array(A)\n",
    "\n",
    "eigenvals, eigenvecs = np.linalg.eig(A)\n",
    "print(A)\n",
    "print(eigenvals)\n",
    "print(eigenvecs)\n",
    "\n",
    "##\n",
    "# DEMO: RECONSTRUCTING A MATRIX FROM EIGEN PARAMETERS\n",
    "#\n",
    "Q = eigenvecs\n",
    "Lambda = np.diag(eigenvals)\n",
    "A = Q.dot(Lambda).dot(np.linalg.inv(Q))\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87c098f",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# Appendix <a name =\"Section6\"></a>\n",
    "\n",
    "## 1. Singular value decomposition\n",
    "1. Simply put, it is similar to that of eigen decomposition: in eigen decomposition, we decompose a matrix into eigen parameters including eigenvalues and eigen vectors; here, we decompose the matrix into singular parameters including singular vectors and singular values. \n",
    "\n",
    "\n",
    "2. This decomposition allows us to discover some of the same kind of information as the eigendecomposition. However, the singular value decomposition is more generally applicable. Every real matrix has a singular value decomposition (SVD), but the same is not true of the eigenvalue decomposition. For example, if a matrix is not square, the eigendecomposition is not defined, and we must use a singular value decomposition instead.\n",
    "\n",
    "\n",
    "3. The SVD is expressed as follows:\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{U}\\mathbf{D}\\mathbf{V}^T\n",
    "$$\n",
    "where that $\\mathbf{A}$ is an m × n matrix. $\\mathbf{U}$ is defined to be an m × m matrix, $\\mathbf{D}$ to be an m × n matrix, and $\\mathbf{V}$ to be an n × n matrix. \n",
    "\n",
    "\n",
    "4. Each of these matrices is defined to have a special structure. The matrices $\\mathbf{U}$ and $\\mathbf{V}$ are both defined to be orthogonal matrices. The matrix $\\mathbf{D}$ is defined to be a diagonal matrix. \n",
    "\n",
    "\n",
    "5. The elements along the diagonal of $\\mathbf{D}$ are known as the singular values of the matrix $\\mathbf{A}$. The columns of $\\mathbf{U}$ are known as the left-singular vectors. The columns of $\\mathbf{V}$ are known as as the right-singular vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fb1c05",
   "metadata": {},
   "source": [
    "## Suggestive Readings <a name =\"Section7\"></a>\n",
    "\n",
    "1. The Youtube channel _3 Blue 1 Brown_ provides a comprehensive list of animated essential linear algebra topics: [Essence of linear algebra](https://youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
