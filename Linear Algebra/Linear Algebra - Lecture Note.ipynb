{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c28307bc",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Linear Algebra\n",
    "Author: Vo, Huynh Quang Nguyen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044a239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ffb1b3",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Acknowledgements:\n",
    "The contents of this note are based on the lecture notes and the materials from the following source:\n",
    "1. _Mathematics for Machine Learning_ course given by Prof. David Dye, Dr. Samuel J. Cooper, and Dr. A. Freddie Page from Imperial College London. Available in Coursera.\n",
    "2. _Essential Math for Data Science in 6 Weeks_ webinar given by Dr. Thomas Nield. Available in O'Reily Learning.\n",
    "3. _Deep Learning_ book by Dr. Ian Goodfellow, Prof. Yoshua Bengio, and Prof. Aaron Courville. The book is available for public access via an designated website.\n",
    "4. _ELEC-E3240 Photonics_ given by Prof. Zhipei Sun and Huynh Quang Nguyen Vo (Teaching Assistant)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6eecb4",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Table of Contents\n",
    "1. [Introduction to Linear Algebra](#Section1) \n",
    "2. [Vector operations](#Section2) \n",
    "3. [Matrix operations](#Section3) \n",
    "4. [Solving systems of equations](#Section4)\n",
    "5. [Eigenvalues and Eigenvectors](#Section5)\n",
    "6. [Appendix](#Section6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc07a10f",
   "metadata": {},
   "source": [
    "## I. Introduction to Linear Algebra <a name = 'Section1'></a>\n",
    "\n",
    "### 1. Why we have to learn?\n",
    "\n",
    "* Firstly, linear algebra can be found in many areas of science, technology, engineering, and data science. \n",
    "* Secondly, linear algebra is also the backbone of machine learning, data management, graphical modelling, and other computer science areas.\n",
    "* Thirdly, modern computers model data as vectors and matrices to perform operations more effectively.\n",
    "\n",
    "Therefore, linear algebra is a **MUST** to advance your knowledge in machine learning, statistical modelling, or other areas in computer science.\n",
    "\n",
    "\n",
    "### 2. Applications of Linear Algebra\n",
    "\n",
    "As mentioned earlier, linear algebra serves as a backbone in many scientific fields. Below are selected examples of how it is applied.\n",
    "\n",
    "#### Optics\n",
    "\n",
    "<div>\n",
    "<img src=\"images/ray-transfer-matrix.png\" width=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 1: Ray transfer matrix analysis for imaging inside linear media. This method of image computation is widely employed when designing an optical system such as a microscope, a telescope, or a laser guidance system.\n",
    "\n",
    "#### Computer vision\n",
    "\n",
    "<div>\n",
    "<img src=\"images/perspective.png\" width=\"70%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 2: Perspective transformation from 3D into 2D (adapted from **Computer Vision: Algorithms and Applications** by Szeliski). This method of projection is popular in computer graphics when recreating 3D models in 2D environments.\n",
    "\n",
    "\n",
    "![SegmentLocal](images/2D_Convolution_Animation.gif)\n",
    "Figure 3: Operating principles of a 2-D convolution. The convolution operation serves as a backbone in mask-based image filtering, and convolutional neural network. \n",
    "\n",
    "#### Geography\n",
    "<div>\n",
    "<img src=\"images/geography.png\" width=\"50%\"/>    \n",
    "</div>\n",
    "\n",
    "Figure 4: Computation of an area of a polygon using coordinates. This method, which is often referred to as the shoelace algorithm or Gaussian area algorithm, is widely employed in computational geography for computing the area of a region.\n",
    "\n",
    "#### Signal Processing\n",
    "<div>\n",
    "<img src=\"images/signal.png\" width=\"35%\" />\n",
    "</div>\n",
    "Figure 4: Operating principles of a digital filter. A signal in the time-domain is transformed to frequency-domain using Fourier transform, then subjected to a digital filter to smoothen out, and finally converted back to the time-domain. This concept of filtering in frequency-domain can also be applicable for image processing.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c2b5a4b",
   "metadata": {},
   "source": [
    "## II. Vector operations\n",
    "\n",
    "### 1. What is a vector:\n",
    "1. Vectors are an arrow in space, with a specific direction and length. This can mean different things to different disciplines:\n",
    "    * Physics - direction and magnitude (e.g velocity)\n",
    "    * Computer Science - an array of data.\n",
    "    * Math - A direction and scale on a designated coordinate system.\n",
    "\n",
    "\n",
    "2. For a two-dimensional vector, we can think of it as a pair of numbers. For example, we can interpret the following vector $\\hat{v} = [3,4] $ as follows:\n",
    "    * This vector has the name of $\\hat{v}$.\n",
    "    * From the origin (0,0) we make 3 steps along the x-axis, and 4 steps up the y-axis. \n",
    "\n",
    "\n",
    "3. In practice, vectors must start at the origin (0,0) and cannot arbitrarily start at any point in space.\n",
    "\n",
    "<div>\n",
    "<img src=\"images/vector-representation.png\" width=\"100%\"/>\n",
    "</div>\n",
    "Figure 1: Examples of 2D vectors.\n",
    "\n",
    "4. Vectors can apply beyond just a 2-dimensional space. In practice, vectors can exist in any number of dimensions, although it gets hard to visualize outside of a 3-dimension space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a818b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 5]\n",
      "(3,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "# DEMO: DECLARING A VECTOR\n",
    "#\n",
    "\n",
    "v = [3, 4, 5]\n",
    "v = np.array(v)     # We use this function to convert a Python list into a vector\n",
    "print(v)\n",
    "print(v.shape)      #We use this function to determine the dimension of our vector\n",
    "\n",
    "##\n",
    "# DEMO: VECTOR VISUALIZATION\n",
    "#\n",
    "origin = np.array([0,0])  \n",
    "v = np.array([3,2])\n",
    "\n",
    "# Creating plot\n",
    "plt.quiver(origin[0], origin[1], v[0], v[1], color='b', units='xy', scale=1)  \n",
    "plt.xlim(-2, 5)\n",
    "plt.ylim(-2, 2.5)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b90d78b7",
   "metadata": {},
   "source": [
    "### 2. Basic operations with vectors\n",
    "\n",
    "#### a) Vector-on-vector addition and subtraction\n",
    "1. To visually add these two vectors together, connect one vector after the other and walk to the tip of the last vector. The point we end at is a new vector, the result of summing the two vectors. \n",
    "\n",
    "\n",
    "2. Because of the commutative nature of adding vectors, as it does not matter which order we add them.\n",
    "\n",
    "$$\n",
    "\\hat{v} + \\hat{w} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} + \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3. For subtraction, we can imagine it as a vector $\\hat{v}$ sums with a 'negative' vector $-\\hat{w}$ out of the original $\\hat{w}$. Noted that the 'negative' notion here means a vector having the same length but with an opposite direction.\n",
    "\n",
    "<div>\n",
    "<img src=\"images/vector-add.png\" width=\"100%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 2: An example of adding two vectors together by connecting one vector after the other and walking to the tip of the last vector.\n",
    "\n",
    "#### b) Vector-on-vector Multiplication\n",
    "1. We can also grow/shrink a vector by multiplying (others referred to this as scaling, though it is a matter of word choice) with a scalar value.\n",
    "\n",
    "<div>\n",
    "<img src = \"images/vector-multiply.png\" width = \"80%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 3: Examples of scaling a vector by a scalar: positive scalars increase/decrease the length of a vector (a-b), while negative scalars not only increase/decrease but also reverse a direction of a vector (c). \n",
    "\n",
    "#### c) Magnitude and Normalization\n",
    "1. A magnitude (length) of a vector can be computed as follows:\n",
    "$$\n",
    "||\\hat{v}|| = \\sqrt{v_1^2 + v_2^2 + ... + v_n^2}\n",
    "$$\n",
    "\n",
    "2. A vector can be normalized by dividing its elements by their magnitude:\n",
    "\n",
    "$$\n",
    "\\hat{v}_{norm} = \\frac{\\hat{v}}{||\\hat{v}||} = \\frac{1}{\\sqrt{v_1^2 + v_2^2 + ... + v_n^2}}\\begin{bmatrix} v_1\\\\ v_2 \\\\ ... \\\\v_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3. The above equation that computes the length of a vector is the so-called $L^2$ normalization also known as the **Euclidean** normalization. In practice, a vector can be subjected to $L^p$ normalization with $p\\ge1$:\n",
    "$$\n",
    "||\\hat{v}||_p = (\\sum_{i} |v_i|^p)^{1/p}\n",
    "$$\n",
    "\n",
    "#### d) Dot product and cross product\n",
    "1. Dot product, also known as scalar product or inner product, is a pair-wise operation between two vectors and returns a single number. Geometrically speaking in 2D and 3D, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them.\n",
    "\n",
    "$$\n",
    "\\hat{v} \\bullet \\hat{w} = ||\\hat{v}||_2||\\hat{w}||_2cos(\\hat{v},\\hat{w})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{v} \\bullet \\hat{w} = v_1w_1 + v_2w_2 + ... + v_nw_n\n",
    "$$\n",
    "\n",
    "2. Cross product is the operation that takes two vectors and returns a vector that is perpendicular to the plane from the aforementioned vectors.\n",
    "$$\n",
    "\\hat{v} \\times \\hat{w} = ||\\hat{v}||_2||\\hat{w}||_2cos(\\hat{v},\\hat{w})\n",
    "$$\n",
    "\n",
    "<div>\n",
    "<img src =\"images/dot-cross-product.png\" width =\"80%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 4: Visualization of a dot product and a cross product between vector $\\hat{A}$ and $\\hat{B}$. The angle between $\\hat{A}$ and $\\hat{B}$ is $\\theta$. Notice the notion $\\hat{e}$ means the result of a cross product is a vector.\n",
    "\n",
    "#### e) Scalar projection\n",
    "1. Scalar projects are the size of the “shadow” of a vector onto another vector, if we imagine the sun shining down perpendicular to the second vector. Vector projections are that shadow vector. As the names imply, scalar projects are scalars and vector projections are vectors.\n",
    "\n",
    "<div>\n",
    "<img src =\"images/scalar-projection.png\" width =\"30%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 5: An example of a vector $\\hat{s}$ projected onto another vector $\\hat{r}$ resulting a vector $\\hat{s_1}$.\n",
    "\n",
    "2. The scalar projection of $\\hat{s}$ onto $\\hat{r}$ is calculated as :\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{r}\\bullet\\hat{s}}{||\\hat{r}||_2}\n",
    "$$ \n",
    "\n",
    "3. The vector projection is calculated as:\n",
    "\n",
    "$$\\hat{r}\\frac{\\hat{r}\\bullet\\hat{s}}{r^2}\n",
    "$$\n",
    "\n",
    "For example, we want to project on $\\hat{s} = \\begin{bmatrix} 10 \\\\ 5 \\\\ -6 \\end{bmatrix}$ onto $\\begin{bmatrix} 3 \\\\ -4 \\\\ 0 \\end{bmatrix}$.\n",
    "* The resulting scalar projection is $\\frac{\\hat{r}\\bullet\\hat{s}}{||\\hat{r}||_2} = \\frac{10}{5} = 2 $\n",
    "* The resulting vector projection is $\\hat{r}\\frac{\\hat{r}\\bullet\\hat{s}}{r^2} = \\begin{bmatrix} 3 \\\\ -4 \\\\ 0\\end{bmatrix} \\frac{10}{25} = \\begin{bmatrix} 6/5 \\\\ -8/5 \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "#### f) Basis vectors, basis decomposition and basis transformation\n",
    "1. A vector can be decomposed and represented by its basis vectors. For example, consider a vector $\\hat{v} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}$, it can be decompose into:\n",
    "$$\n",
    "\\hat{v} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} = 3\\hat{i} + 2\\hat{j} = 3\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + 2\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "2. Thus, we can understand that the basis vectors are the “reference points” we use to describe other vectors. A basis vector having a length of 1 and point in perpendicular positive directions is called the **standard basis vector**. In 2D, two **standard basis vectors** (one for the x-axis and the other for the y-axis) can be combined to form a so-called **identity matrix**.\n",
    "\n",
    "$$\n",
    "\\hat{i}\\hat{j} = \\begin{bmatrix} 1 & 0\\\\ 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The basis vector concept can be extended beyond 2D ($\\hat{i}$, $\\hat{j}$) to 3D ($\\hat{i}$, $\\hat{j}$, $\\hat{k}$), and to\n",
    "as many additional dimensions we need. As a result, we can expand the above **identity matrix** to a multi-dimensional **identity matrix**.\n",
    "\n",
    "$$\n",
    "\\mathbf{I} = \\begin{bmatrix} 1 & 0 & 0 & ... & 0 \\\\ 0 & 1 & 0 & ...  & 0 \\\\ 0 & 0 & 1 & ... & 0 \\\\ ... & ... & ... & ... & ... \\\\ 0 & 0 & 0 & ... & 1  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Overall, the basis vectors can be anything as long as they are **orthogonal** to each other, which means their dot product is zero.\n",
    "\n",
    "3. To conduct a basis decomposition of a vector $\\hat{v}$ onto a set of basis vectors $\\hat{b_1}$ and $\\hat{b_2}$, we can simply use:\n",
    "$$\n",
    "v'_{b_1} = \\frac{\\hat{b_1}\\bullet\\hat{v}}{b_1^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "v'_{b_2} = \\frac{\\hat{b_2}\\bullet\\hat{v}}{b_2^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{v}'= \\begin{bmatrix} v'_{b_1} \\\\ v'_{b_2} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If the basis vectors are the standard basis vectors, then it is very straightforward. \n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"images/basis-vector.png\" width=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 6: Visualization of two standard basis vectors $\\hat{i}$ and $\\hat{j}$ (a), and how a vector is represented using two basis vectors (b).\n",
    "\n",
    "4. Basis transformation, to put it simply, is changing from one set of reference points to another set of reference points. This transformation usually involves a matrix.  \n",
    "\n",
    "<div>\n",
    "<img src=\"images/basis-transformation.png\" width=\"20%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 7: Visualization of two different sets of basis vectors. They can be transformed to one another by using basis transformation.\n",
    "\n",
    "#### d) Span and linearly independent\n",
    "1. These vectors $\\hat{v}$ and $\\hat{w}$, fixed in two different directions, can be scaled and added to create **any** new vector $\\alpha\\hat{v} + \\beta\\hat{w}$. \n",
    "\n",
    "\n",
    "2. The entire space of vectors we can create from two vectors fixed in direction but allowed to scale is known as **span**. \n",
    "\n",
    "\n",
    "3. Therefore, if the sum of these two vectors gives us access to all vectors in space, they are called **linearly independent**.\n",
    "\n",
    "<div>\n",
    "<img src=\"images/linear-independent.png\" width=\"100%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 8: Examples of span and linearly independence among two vectors: only the first three examples are valid in term of span because the direction of each vector is fixed.\n",
    "\n",
    "\n",
    "4. Characteristics of linear independency:\n",
    "    * With a 3D vector space, if two vectors are linearly dependent (share a direction) but the third vector is linearly independent of the other two, the span will be a flat plane in space. \n",
    "    * If all three vectors were linearly dependent, their span would only be a line in space. \n",
    "    * This concept applies to any number of dimensions, not just 2 or 3.\n",
    "    \n",
    "<div>\n",
    "<img src=\"images/linear-dependent.png\" width = \"50%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 9: An examples of three linearly independent vectors forming a span that is a 3D space (a); if one or two of them are linear dependent, their span will be a flat plane in space.\n",
    "\n",
    "\n",
    "5. We can determine whether a set of vectors are linear independent using the following steps:\n",
    "    * For example, we have three vectors to consider $\\hat{v}_1 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 5 \\end{bmatrix} $, $\\hat{v}_2 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 8 \\end{bmatrix} $, $\\hat{v}_3 = \\begin{bmatrix} 4 \\\\ -1 \\\\ 0 \\end{bmatrix} $. To determine the linear independecy, we need to find a set of scalars that fulfills\n",
    "$$\n",
    "c_1 \\begin{bmatrix} 0 \\\\ 1 \\\\ 5 \\end{bmatrix} + c_2 \\begin{bmatrix} 1 \\\\ 2 \\\\ 8 \\end{bmatrix} + c_3 \\begin{bmatrix} 4 \\\\ -1 \\\\ 0 \\end{bmatrix} = 0 \n",
    "$$\n",
    "    * After solving this, the only set of scalars that fulfills this equation is $c_1 = c_2 = c_3 = 0$. Therefore, we can conclude that these vectors are linear indepedent.\n",
    "    * An easier method is forming a matrix containing the vectors-under-interest, then finding its matrix determinant. If the determinant is zero, then some or all vectors-under-interest are linear dependents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5799771a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 1]\n",
      "[-4.5 -3. ]\n",
      "32\n",
      "[-3  6 -3]\n",
      "[0.4, 2.2]\n",
      "These vectors are linear independent.\n"
     ]
    }
   ],
   "source": [
    "v = np.array([3, 2])\n",
    "w = np.array([2, -1])\n",
    "\n",
    "##\n",
    "# DEMO: VECTOR-ON-VECTOR ADDITION\n",
    "#\n",
    "add = v + w\n",
    "print(add)\n",
    "\n",
    "##\n",
    "# DEMO: VECTOR-ON-VECTOR MULTIPLICATION\n",
    "#\n",
    "mul = -1.5 * v\n",
    "print(mul)\n",
    "\n",
    "##\n",
    "# DEMO: DOT PRODUCT AND CROSS PRODUCT\n",
    "#\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([4, 5, 6])\n",
    "dot = np.dot(x,y)\n",
    "cross = np.cross(x,y)\n",
    "print(dot)\n",
    "print(cross)\n",
    "\n",
    "##\n",
    "# DEMO: BASIC DECOMPOSITION\n",
    "#\n",
    "i_hat = np.array([3, 4])\n",
    "j_hat = np.array([4, -3])\n",
    "v = np.array([10,-5])\n",
    "\n",
    "w = [np.dot(i_hat,v) / np.linalg.norm(i_hat) ** 2, \n",
    "     np.dot(j_hat,v) / np.linalg.norm(j_hat)** 2]\n",
    "print(w)\n",
    "\n",
    "##\n",
    "# DEMO: LINEAR INDEPENDENCE\n",
    "#\n",
    "v_1 = np.array([0,1,5])\n",
    "v_2 = np.array([1,2,8])\n",
    "v_3 = np.array([4,-1,0])\n",
    "\n",
    "v = np.array([v_1, v_2, v_3]).transpose()\n",
    "result = np.linalg.det(v)\n",
    "\n",
    "if not result == 0:\n",
    "    print('These vectors are linear independent.')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dad36c",
   "metadata": {
    "editable": false
   },
   "source": [
    "## II. Matrix operations <a name = \"Section2\" ></a>\n",
    "\n",
    "### 1. What is a matrix:\n",
    "1. A matrix is like a vector (or collection of vectors), and it can have multiple rows and columns. \n",
    "2. In practice, matrix is a convenient way to package data. For example, it is more convenient to represent a system of equations as matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94478743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4 5]\n",
      " [6 7 8]]\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# DEMO: DECLARE A MATRIX\n",
    "#\n",
    "v = [[3,4,5], [6,7,8]]\n",
    "v = np.array(v)\n",
    "print(v)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37c6c3",
   "metadata": {},
   "source": [
    "### 2. Basic matrix operations:\n",
    "\n",
    "#### a. Matrix-on-vector multiplication\n",
    "1. Recall the basis transformation, the basis transformation is technically speaking a matrix-on-vector multiplication.\n",
    "$$\n",
    "\\begin{bmatrix} x_{new} \\\\ y_{new} \\end{bmatrix} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} x_{old} \\\\ y_{old} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For example, finding an image (output) of an object formed by an optical system is just applying matrix-vector multiplication. Here, our vectors are an object and its corresponding image, and the matrix is the transformation matrix (**M**) of the system.\n",
    "\n",
    "<div>\n",
    "<img src =\"images/matrix-vector.png\" width=\"30%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 1: An example of matrix-on-vector multiplication: finding the output image of an object created by an optical system (adapted from **Fundamentals of Photonics** by Saleh et al.)\n",
    "\n",
    "Another example is vector rotation by an angle of $\\theta$ described as: $\\hat{v} = \\hat{u}\\begin{bmatrix} cos\\theta & -sin\\theta \\\\ sin\\theta  & cos\\theta \\end{bmatrix}$\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"images/basis.png\" width = \"30%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 2: Rotation of a vector $\\hat{u}$ into a vector $\\hat{v}$ by an angle $\\theta$.\n",
    "\n",
    "\n",
    "2. Matrix-vector multiplication is done by computing the dot product between a matrix and a vector:\n",
    "\n",
    "<div>\n",
    "<img src = \"images/dot-produc-matrix.png\" width= \"50%\"/>\n",
    "</div>\n",
    "\n",
    "Figure 3: How the dot product between a matrix and a vector works.\n",
    "\n",
    "3. Using this concept, we can describe a wide variety of transformations such as rotations, reflections, dilations, and much more.\n",
    "\n",
    "#### b. Linear Transformation\n",
    "\n",
    "1. To make things simple, let's consider the rotation of basis $(\\hat{u_1},\\hat{u_2})$ to the new basis $(\\hat{v_1},\\hat{v_2})$ by an angle $\\theta$. Here, the basis notation means 'a reference system' consisting of 'reference points' that are our basis vectors.\n",
    "\n",
    "<div>\n",
    "<img src = \"images/linear_transform.png\" width= \"25%\"/>\n",
    "</div>\n",
    "\n",
    "We know that this basis rotation is simply put a basis transformation:\n",
    "$$\n",
    "(\\hat{v_1},\\hat{v_2})  =  (\\hat{u_1},\\hat{u_2}) \\begin{bmatrix} cos\\theta & -sin\\theta \\\\ sin\\theta  & cos\\theta \\end{bmatrix}\n",
    "$$    \n",
    "\n",
    "Therefore, we can simply describe this using this formula:\n",
    "$$\n",
    "(\\hat{v_1},\\hat{v_2})  =  (\\hat{u_1},\\hat{u_2})(\\mathbf{u}→\\mathbf{v})\n",
    "$$\n",
    "\n",
    "Because of the basis transformation, we can describe a changes of a coordinates of a vector from an old basis $(\\hat{u_1},\\hat{u_2})$ to a new basis $(\\hat{v_1},\\hat{v_2})$ as: \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} x_{new} \\\\ y_{new} \\end{bmatrix} = \\begin{bmatrix} cos\\theta & -sin\\theta \\\\ sin\\theta  & cos\\theta \\end{bmatrix} \\begin{bmatrix} x_{old} \\\\ y_{old} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "or in this formula:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{\\mathbf{v}} = (\\mathbf{u}→\\mathbf{v})\\mathbf{x}^{\\mathbf{u}}\n",
    "$$\n",
    "\n",
    "\n",
    "Next, imagine that we fix our old basis $(\\hat{u_1},\\hat{u_2})$ and now represent all vectors with this basis. So, we have a vector $\\hat{x}$ then we rotate it by an angle $\\theta$ to transform into a vector $\\hat{y}$. Now, instead of describing the vector $\\hat{y}$ with a new basis $(\\hat{v_1},\\hat{v_2})$, we describe that vector with our own fixed basis. This description is called linear transformation, and can be simply described with the formula:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}^{\\mathbf{u}} = (\\mathbf{u}→\\mathbf{v})\\mathbf{x}^{\\mathbf{u}}\n",
    "$$\n",
    "\n",
    "2. Think of linear transformation as tracking where the basis vectors land, and then we can use that to transform a vector also. This concept of seeing where the basis vectors land is important because it allows us not just to create vectors but also transform existing vectors.\n",
    "\n",
    "For example, we have this vector $\\hat{v} = \\begin{bmatrix} 4 \\\\ 2 \\end{bmatrix}$, and we transformed it somehow and it became $\\hat{w}$. The basis vectors $\\hat{i}$ and $\\hat{j}$ now land at $\\begin{bmatrix} -0.25 \\\\ 0 \\end{bmatrix}$ and $\\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}$, respectively. We can describe the new vector with our fixed basis ($\\hat{i}$, $\\hat{j}$) as:\n",
    "\n",
    "$$\n",
    "\\hat{w} = \\begin{bmatrix} -0.25 & 0 \\\\ 0 & -1 \\end{bmatrix} \\hat{v} = \\begin{bmatrix} -1 \\\\ -2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "$$\n",
    "\n",
    "<div>\n",
    "<img src=\"images/lineartransformation.png\" width = \"100%\" />\n",
    "</div>\n",
    "\n",
    "Figure 4: An example of the linear transformation: linear transformation is tracking where the basis vectors land and use them to describe a transformed vector (a-b); using this concept, we can describe nearly almost all typical linear transformations such as sheer, rotation, flip, scale, etc.\n",
    "\n",
    "#### c. Determinant\n",
    "1. When we perform linear transformations, we are expanding or reducing space, and the degree of this expansion or reduction is called **determinant**. In other words, **determinants** describe how much a sampled area in vector space changes in scale with linear transformations.\n",
    "\n",
    "\n",
    "2. Characteristics of determinant:\n",
    "    * Scaling will increase or decrease the determinant, as that will increase/decrease the sampled area. Shear or rotation do not affect the magnitude of the determinant.\n",
    "    * When the orientation flips (for example, $\\hat{i}$ and $\\hat{j}$ swap clockwise positions) then the determinant will be negative.\n",
    "    * When a determinant is 0, that means the transformation is linearly dependent and has squished all of space into a line. Because at this point, there is no area! This is why testing whether a determinant is zero tells us about linear dependency.\n",
    "    * As usual, the determinant extends to 3 or more dimensions. At higher dimensions, it becomes a matter of visualizing a sampled volume scaling, rotating, sheering, and flipping.\n",
    "    \n",
    "<div>\n",
    "<img src=\"images/determinants.png\" width=\"100%\">\n",
    "</div>\n",
    "\n",
    "Figure 5: Determinants and their characteristics: a determinant is simply how much a sampled area in vector space changes in scale with linear transformations (a); shear or rotation do not affect the determinant (b-c); if a determinant is zero, then the sampled area formed by two vectors is zero meaning they are linearly dependent.\n",
    "\n",
    "#### d. Matrix-on-Matrix Multiplication\n",
    "1. We can think of matrix multiplication as applying multiple transformations to a vector space. In other words, we can think of each transformation matrix as a function, where we apply from the inner-most and then outwards. \n",
    "\n",
    "For example, we have a vector $\\hat{v}$, and we want to apply the rotation then a sheer transformations to this vector:\n",
    "* The rotation transformation is described as: $\\mathbf{r} = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$.\n",
    "* The shear transformation is described as: $\\mathbf{s} = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$.\n",
    "* The combined rotation and shear is: $\\mathbf{M} = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$. Notice the order of the matrices.\n",
    "* Therefore, the new vector $\\hat{w}$ is:\n",
    "$\\hat{w} = \\begin{bmatrix} x_{new} \\\\ y_{new} \\end{bmatrix} = M \\begin{bmatrix} x_{old} \\\\ y_{old} \\end{bmatrix} $\n",
    "\n",
    "<div>\n",
    "<img src=\"images/matrix-matrix-mul.png\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "Figure 5: An example of how rotation and shear transformations look like. A combined transformation of rotation and shear is simply matrix-matrix multiplication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ff3d971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 -1]\n",
      " [ 1  0]]\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# DEMO: MATRIX-ON-MATRIX MULTIPLICATION\n",
    "#\n",
    "R = np.array([[0, -1],[1,0]])\n",
    "S = np.array([[1,1],[0,1]])\n",
    "\n",
    "M = np.dot(S,R)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ae2bbf",
   "metadata": {
    "editable": false
   },
   "source": [
    "## IV. Solving a system of linear equations <a name = \"Section4\"></a>\n",
    "1. We can represent a system of linear equations as a set of matrices.\n",
    "\n",
    "For example, we have a system of equation like this:\n",
    "$$\n",
    "4x + 2y + 4z = 44\n",
    "$$\n",
    "$$\n",
    "5x + 3y + 7z = 56\n",
    "$$\n",
    "$$\n",
    "9x + 3y + 6z = 72\n",
    "$$\n",
    "* We need to solve for x, y and z so we can rewrite our system as: $ \\mathbf{A} = \\begin{bmatrix} 4 & 2 & 4 \\\\ 5 & 3 & 7 \\\\ 9 & 3 & 6 \\end{bmatrix}$, $\\mathbf{B} = \\begin{bmatrix} 44 \\\\ 56 \\\\ 72 \\end{bmatrix}$, $\\mathbf{C} = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}$\n",
    "* We know that: $\\mathbf{A}\\mathbf{X} = \\mathbf{B}$. Therefore, we can rewrite $\\mathbf{X}$ as function between $\\mathbf{A}$ and $\\mathbf{B}$ as: $\\mathbf{X} = \\mathbf{A}^{-1}\\mathbf{B}$, with $\\mathbf{A}^{-1}$ is the inverse matrix of $\\mathbf{A}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf54ab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2. 34. -8.]\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# DEMO: SOLVING A SYSTEM OF LINEAR EQUATIONS:\n",
    "#\n",
    "A = np.array([[4,2,4],[5,3,7],[9,3,6]])\n",
    "B = np.array([44,56,72]).transpose()\n",
    "X = np.dot(np.linalg.inv(A),B)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e0512f",
   "metadata": {},
   "source": [
    "## V. Eigen values and eigen vectors <a name = \"Section5\"></a>\n",
    "\n",
    "###  1. Matrix decomposition\n",
    "1. It is the method of breaking up a matrix into its basic components. Matrix decomposition is helpful for tasks like finding inverse matrices, calculating determinants, as well as linear regression.\n",
    "\n",
    "2. There are many ways to decompose a matrix, but the most common method is eigendecomposition, which is often used for machine learning and Principal Component Analysis (PCA). \n",
    "\n",
    "### 2. Eigen decomposition\n",
    "1. For a square matrix **A**, an eigenvector ($\\hat{v}$) and eigenvalue ($\\lambda$) make this equation true:\n",
    "$$\n",
    "\\mathbf{A}\\hat{v} = \\lambda\\hat{v}\n",
    "$$\n",
    "\n",
    "For example, we have a square matrix **A** = $\\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix}$. Its eigenvector and eigenvalue is $\\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix}$ and 6, respectively.\n",
    "\n",
    "* Compute the left-hand size (LHS) of the equation: $\\mathbf{A}\\hat{v} = \\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24 \\end{bmatrix} $\n",
    "* Compute the right-hand size (RHS) of the equation: $\\lambda\\hat{v} = 6 \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24 \\end{bmatrix} $\n",
    "* Therefore, the equation is fulfilled.\n",
    "\n",
    "2. To put it simply, eigen decomposition is a type of transformation to find the eigenvalues and eigenvectors.\n",
    "\n",
    "3. We assume that the eigenvector is non-zero.\n",
    "\n",
    "### 3. Finding eigen parameters:\n",
    "1. First, we find the eigenvalue by manipulating our equation:\n",
    "$$\n",
    "\\mathbf{A}\\hat{v} = \\lambda\\hat{v}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\hat{v} - \\lambda I\\hat{v} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{det}(\\mathbf{A} - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "For example, we have a square matrix **A** = $\\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix}$, then the final equation becomes:\n",
    "$$\n",
    "\\mathbf{det}(\\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} - \\lambda\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{det}(\\begin{bmatrix} -6 - \\lambda & 3 \\\\ 4 & 5 - \\lambda \\end{bmatrix}) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "(-6 -\\lambda)(5-\\lambda) - 3\\times4 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda^2 + \\lambda - 42 = 0\n",
    "$$\n",
    "\n",
    "From here, we get two possible eigenvalues being -7 and 6.\n",
    "\n",
    "2. Using the eigenvalues, we can calculate our respective eigenvectors by substituting them with the original equation.\n",
    "\n",
    "Using the above example, for the eigenvalue of 6 we have:\n",
    "$$\n",
    "\\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = 6 \\begin{bmatrix} x \\\\ y \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} - 6\\begin{bmatrix} 1 & 0 \\\\ 0 & 1  \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} -12 & 3 \\\\ 4 & -1 \\end{bmatrix}\\begin{bmatrix} x \\\\ y \\end{bmatrix} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### 4. Why eigen parameters?\n",
    "1. Finding the eigen parameters means we are finding a vector that when our matrix-under-interest multiplies with the former, the matrix does not change its direction!\n",
    "\n",
    "2. The only thing that changes is the 'size' of the matrix:\n",
    "    * 1 means no change,\n",
    "    * 2 means doubling in length,\n",
    "    * −1 means pointing backwards along the eigenvalue's direction.\n",
    "\n",
    "![SegmentLocal](images/Eigenvectors.gif)\n",
    "\n",
    "Figure 1: An example of how eigenvectors operate: when a matrix is transformed using an eigenvector (blue vectors), the only thing that changes is its 'size'. The same goes when this matrix is transformed by another eigenvector (purple ones).\n",
    "\n",
    "\n",
    "### 5. Reconstructing a matrix from eigen parameters.\n",
    "1. From the original equation, we can reconstruct a matrix from eigen parameters using:\n",
    "$$\n",
    "A = Q\\Lambda Q^{-1}\n",
    "$$\n",
    "in which Q is the matrix of eigenvectors, $\\Lambda$ is the eigenvalues in diagonal form, and $Q^{-1}$ is the inverse of the matrix.\n",
    "\n",
    "For example, we have a matrix **A** = $\\begin{bmatrix} 1 & 2 \\\\ 4 & 5 \\end{bmatrix}$, and we manage to find its eigenvalues to be -0.464 and 6.464, its eigenvectors to be $\\begin{bmatrix} 0.806 \\\\ 0.343\\end{bmatrix}$ and $\\begin{bmatrix} 0.59 \\\\ -0.939 \\end{bmatrix}$, respectively.\n",
    "\n",
    "* We rewrite our eigenvalues and eigenvectors. For eigenvalues: $\\begin{bmatrix} -0.464 \\\\ 6.464 \\end{bmatrix}$, while for eigenvectors: $\\begin{bmatrix} 0.806 & 0.343 \\\\ 0.59 & -0.939\\end{bmatrix}$ \n",
    "* We apply the equation to reconstruct **A**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57960fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [4 5]]\n",
      "[-0.46410162  6.46410162]\n",
      "[[-0.80689822 -0.34372377]\n",
      " [ 0.59069049 -0.9390708 ]]\n",
      "[[1. 2.]\n",
      " [4. 5.]]\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# DEMO: FINDING EIGEN PARAMETERS\n",
    "#\n",
    "A = [[1,2], [4,5]]\n",
    "A = np.array(A)\n",
    "\n",
    "eigenvals, eigenvecs = np.linalg.eig(A)\n",
    "print(A)\n",
    "print(eigenvals)\n",
    "print(eigenvecs)\n",
    "\n",
    "##\n",
    "# DEMO: RECONSTRUCTING A MATRIX FROM EIGEN PARAMETERS\n",
    "#\n",
    "Q = eigenvecs\n",
    "Lambda = np.diag(eigenvals)\n",
    "A = Q.dot(Lambda).dot(np.linalg.inv(Q))\n",
    "print(A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c87c098f",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### 1. Rediscovering dot product\n",
    "1. We know that dot product is an operation that does a multiply/add operation between elements in vectors and matrices. So exactly what does it try to accomplish?\n",
    "\n",
    "2. We can think of the dot product between two vectors as taking one vector’s length, and projecting another vector onto it and multiplying that resulting vector’s length.\n",
    "For example, we project $\\hat{w}$ directly onto $\\hat{v}$. The dot product between $\\hat{w}$ and $\\hat{v}$ is a scalar that is the product between the projection of $\\hat{w}$'s length and the $\\hat{v}$'s length.\n",
    "\n",
    "<div>\n",
    "<img src =\"images/dot-product.png\" width = 80%>\n",
    "</div>\n",
    "\n",
    "Figure 1: The operation of a dot product between two vectors.\n",
    "\n",
    "3. For the dot product between two matrices, we can think it as the the means of executing linear transformations, by means of several projections.  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
